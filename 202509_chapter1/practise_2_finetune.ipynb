{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6452314",
   "metadata": {},
   "source": "# 模型微调 - MRPC句子对分类任务\n\n## 任务说明\n- **数据集**: GLUE MRPC (Microsoft Research Paraphrase Corpus)\n- **任务类型**: 句子对二分类（判断两个句子是否语义相同）\n- **模型**: BERT-base-uncased（预训练模型）+ 分类头\n- **微调目标**: 让BERT学会判断句子对的语义相似性"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6733dc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "DataCollatorWithPadding(tokenizer=BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "print(tokenized_datasets[\"train\"].column_names)\n",
    "print(data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "id": "c5xq451gu0r",
   "source": "# 查看数据集示例\nprint(\"数据集示例：\")\nprint(\"=\"*50)\nfor i in range(3):\n    example = raw_datasets[\"train\"][i]\n    print(f\"示例 {i+1}:\")\n    print(f\"句子1: {example['sentence1']}\")\n    print(f\"句子2: {example['sentence2']}\")\n    print(f\"标签: {example['label']} ({'语义相同' if example['label'] == 1 else '语义不同'})\")\n    print(\"-\"*50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7f54fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c3d5bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80irscc170r",
   "source": "## 模型架构说明\n\nBERT模型的不同用途：\n1. **BertModel**: 原始BERT，输出hidden states\n2. **BertForMaskedLM**: 用于掩码语言模型任务（MLM）\n3. **BertForSequenceClassification**: 用于序列分类任务（本例使用）\n4. **BertForTokenClassification**: 用于token分类（如NER）\n5. **BertForQuestionAnswering**: 用于问答任务",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c26f9391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12a05ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   7/1377 00:00 < 00:34, 39.31 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.4420890170937443, metrics={'train_runtime': 38.0527, 'train_samples_per_second': 289.178, 'train_steps_per_second': 36.187, 'total_flos': 405114969714960.0, 'train_loss': 0.4420890170937443, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fc7e297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe19fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ca22f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8333333333333334, 'f1': 0.8839590443686007}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d71f867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89652e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\"test-trainer\", eval_strategy=\"epoch\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24d75177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1377' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1377/1377 00:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412911</td>\n",
       "      <td>0.840686</td>\n",
       "      <td>0.885764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.528800</td>\n",
       "      <td>0.495231</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.895470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.307100</td>\n",
       "      <td>0.645139</td>\n",
       "      <td>0.865196</td>\n",
       "      <td>0.905336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1377, training_loss=0.35462320586781443, metrics={'train_runtime': 43.9034, 'train_samples_per_second': 250.641, 'train_steps_per_second': 31.364, 'total_flos': 405114969714960.0, 'train_loss': 0.35462320586781443, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb5b0524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408, 2) (408,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8651960784313726, 'f1': 0.9053356282271945}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
    "import numpy as np\n",
    "\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hlu47dme7pp",
   "metadata": {},
   "source": [
    "## 原始模型（未微调）的预测准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elgynd9jk7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载原始模型（未微调）\n",
    "original_model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# 创建用于评估的Trainer（不训练）\n",
    "original_trainer = Trainer(\n",
    "    original_model,\n",
    "    training_args,\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 在验证集上进行预测\n",
    "original_predictions = original_trainer.predict(tokenized_datasets[\"validation\"])\n",
    "print(\"原始模型（未微调）的性能：\")\n",
    "print(f\"Accuracy: {original_predictions.metrics['test_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {original_predictions.metrics['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zbn1r9nmo7b",
   "metadata": {},
   "source": [
    "## 性能对比总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "k9dalg1rqds",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "模型性能对比：\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 原始模型预测\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m original_preds \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39margmax(original_predictions\u001b[38;5;241m.\u001b[39mpredictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m original_metrics \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39moriginal_preds, references\u001b[38;5;241m=\u001b[39moriginal_predictions\u001b[38;5;241m.\u001b[39mlabel_ids)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. 原始模型（未微调）：\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# 性能对比\n",
    "print(\"=\"*50)\n",
    "print(\"模型性能对比：\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 原始模型预测\n",
    "original_preds = np.argmax(original_predictions.predictions, axis=-1)\n",
    "original_metrics = metric.compute(predictions=original_preds, references=original_predictions.label_ids)\n",
    "\n",
    "print(f\"1. 原始模型（未微调）：\")\n",
    "print(f\"   - Accuracy: {original_metrics['accuracy']:.4f}\")\n",
    "print(f\"   - F1 Score: {original_metrics['f1']:.4f}\")\n",
    "print()\n",
    "\n",
    "print(f\"2. 第一次微调（无evaluation策略）：\")\n",
    "print(f\"   - Accuracy: 0.8333\")\n",
    "print(f\"   - F1 Score: 0.8840\")\n",
    "print(f\"   - Training Loss: 0.4421\")\n",
    "print()\n",
    "\n",
    "print(f\"3. 第二次微调（有evaluation策略）：\")\n",
    "print(f\"   - Accuracy: 0.8652\")\n",
    "print(f\"   - F1 Score: 0.9053\")\n",
    "print(f\"   - Training Loss: 0.3546\")\n",
    "print()\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"提升幅度：\")\n",
    "print(f\"第二次微调相比原始模型：\")\n",
    "print(f\"   - Accuracy提升: {(0.8652 - original_metrics['accuracy']):.4f}\")\n",
    "print(f\"   - F1提升: {(0.9053 - original_metrics['f1']):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79253abb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hflearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}