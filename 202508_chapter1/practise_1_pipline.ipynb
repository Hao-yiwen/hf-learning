{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c85a93",
   "metadata": {},
   "source": [
    "# hfä¸­piplineå­¦ä¹ ç¤ºä¾‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3357e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorchç‰ˆæœ¬: 2.8.0+cu128\n",
      "CUDAæ˜¯å¦å¯ç”¨: False\n",
      "\n",
      "=== NVIDIA-SMIä¿¡æ¯ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoyiwen/miniconda3/envs/hflearning/lib/python3.10/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "# æ£€æŸ¥PyTorchå’ŒCUDAç‰ˆæœ¬\n",
    "print(f\"PyTorchç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDAæ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDAç‰ˆæœ¬: {torch.version.cuda}\")\n",
    "    print(f\"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"GPUæ•°é‡: {torch.cuda.device_count()}\")\n",
    "    print(f\"å½“å‰GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # è·å–GPUçš„compute capability\n",
    "    major, minor = torch.cuda.get_device_capability(0)\n",
    "    print(f\"GPU Compute Capability: {major}.{minor}\")\n",
    "\n",
    "# ç”¨nvidia-smiæŸ¥çœ‹æ˜¾å¡ä¿¡æ¯\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    print(\"\\n=== NVIDIA-SMIä¿¡æ¯ ===\")\n",
    "    for line in result.stdout.split('\\n')[5:10]:\n",
    "        print(line)\n",
    "except:\n",
    "    print(\"æ— æ³•è¿è¡Œnvidia-smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ab4589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoyiwen/miniconda3/envs/hflearning/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.78505939245224}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "classifier(\"I've been waiting for a Hugging Face course my whole life.\")\n",
    "\n",
    "classifier(\"æ‚¨å¥½\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c865ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.08184506744146347,\n",
       "  'token': 3143,\n",
       "  'token_str': 'complete',\n",
       "  'sequence': 'this is a complete text'},\n",
       " {'score': 0.07022914290428162,\n",
       "  'token': 7704,\n",
       "  'token_str': 'partial',\n",
       "  'sequence': 'this is a partial text'},\n",
       " {'score': 0.026181718334555626,\n",
       "  'token': 2460,\n",
       "  'token_str': 'short',\n",
       "  'sequence': 'this is a short text'},\n",
       " {'score': 0.019824033603072166,\n",
       "  'token': 3763,\n",
       "  'token_str': 'latin',\n",
       "  'sequence': 'this is a latin text'},\n",
       " {'score': 0.016259148716926575,\n",
       "  'token': 7099,\n",
       "  'token_str': 'sample',\n",
       "  'sequence': 'this is a sample text'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"distilbert/distilbert-base-uncased\")\n",
    "unmasker(\"This is a [MASK] text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a04b50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹åç§°: distilbert/distilbert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "print(\"æ¨¡å‹åç§°:\", unmasker.model.name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc310f82",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unknown task text-to-image, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m textToImage \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-to-image\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQwen/Qwen-Image\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m textToImage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA beautiful sunset over a calm ocean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/__init__.py:953\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    946\u001b[0m         pipeline_class \u001b[38;5;241m=\u001b[39m get_class_from_dynamic_module(\n\u001b[1;32m    947\u001b[0m             class_ref,\n\u001b[1;32m    948\u001b[0m             model,\n\u001b[1;32m    949\u001b[0m             code_revision\u001b[38;5;241m=\u001b[39mcode_revision,\n\u001b[1;32m    950\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[1;32m    951\u001b[0m         )\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 953\u001b[0m     normalized_task, targeted_task, task_options \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pipeline_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m         pipeline_class \u001b[38;5;241m=\u001b[39m targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/__init__.py:526\u001b[0m, in \u001b[0;36mcheck_task\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_task\u001b[39m(task: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m, Any]:\n\u001b[1;32m    482\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m    Checks an incoming task string, to validate it's correct and return the default Pipeline and Model classes, and\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03m    default models if they exist.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m \n\u001b[1;32m    525\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPIPELINE_REGISTRY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/base.py:1540\u001b[0m, in \u001b[0;36mPipelineRegistry.check_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m task, targeted_task, (tokens[\u001b[38;5;241m1\u001b[39m], tokens[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid translation task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_XX_to_YY\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1540\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, available tasks are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_supported_tasks()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_XX_to_YY\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1542\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unknown task text-to-image, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\""
     ]
    }
   ],
   "source": [
    "textToImage = pipeline(\"text-to-image\", mode=\"Qwen/Qwen-Image\")\n",
    "textToImage(\"A beautiful sunset over a calm ocean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e5b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.8.0+cu128\n",
      "Torchvision: 0.23.0+cu128\n",
      "Transformers: 4.55.4\n",
      "Diffusers: 0.36.0.dev0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import transformers\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Torchvision: {torchvision.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "\n",
    "# å¦‚æœdiffuserså·²å®‰è£…\n",
    "try:\n",
    "    import diffusers\n",
    "    print(f\"Diffusers: {diffusers.__version__}\")\n",
    "except:\n",
    "    print(\"Diffusersæ— æ³•å¯¼å…¥\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d891629c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "classifier(\n",
    "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd739372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9982948899269104}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I've been waiting for a Hugging Face course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a03ffd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoyiwen/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¿»è¯‘ç»“æœ: ä½ å¥½,ä½ ä»Šå¤©å¥½å—?\n",
      "å¥å­ 1: æˆ‘å–œæ¬¢å­¦ä¹ AI\n",
      "å¥å­ 2: ä»Šå¤©å¤©æ°”å¤©æ°”ä¸é”™\n",
      "å¥å­ 3: äººå·¥æ™ºèƒ½(AI)æ­£åœ¨æ”¹å˜ä¸–ç•Œ!\n",
      "å¥å­ 4: Python3.10 082021   (é…æ–¹åŒ¹é…)\n",
      "å¥å­ 5: æ£•è‰²ç‹ç‹¸è·³è¿‡13åªæ‡’ç‹—\n",
      "å¥å­ 6: 2024å¹´,OpenAIçš„GPT-4æ¨¡å‹å–å¾—äº†æ–°çš„é‡Œç¨‹ç¢‘ã€‚\n",
      "å¥å­ 7: è¿™æ˜¯ä¸€æ¬¡æµ‹è¯•! ç¼–å·:12345,ç¬¦å·:______________________________________________________________________________________________________________________________________________________________________\n",
      "å¥å­ 8: è®©æˆ‘ä»¬çœ‹çœ‹æ¨¡å‹æ˜¯å¦‚ä½•æ“ä½œçš„:mojis {{{{{}},ä»£ç ç‰‡æ–­`print('hello')'å’Œ{}}{}}}{}}\n",
      "å¥å­ 9: æ•°æ®éšç§å¾ˆé‡è¦ã€‚ æ‚¨æ˜¯å¦ä¿æŠ¤æ‚¨çš„æ•°æ® ?\n",
      "å¥å­ 10: å®ƒæ”¯æŒè¶…è¿‡100ç§è¯­è¨€ã€‚\n",
      "å¥å­ 11: ä½ èƒ½ç¿»è¯‘å—?\n",
      "å¥å­ 12: å°†æ”¹å˜æœªæ¥,\n"
     ]
    }
   ],
   "source": [
    "# æ­£ç¡®çš„è‹±ä¸­ç¿»è¯‘æ–¹å¼\n",
    "# æ–¹æ³•1ï¼šä½¿ç”¨ Helsinki-NLP çš„ç¿»è¯‘æ¨¡å‹\n",
    "from transformers import pipeline\n",
    "\n",
    "# æŒ‡å®šå…·ä½“çš„è‹±ä¸­ç¿»è¯‘æ¨¡å‹\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "\n",
    "# æµ‹è¯•ç¿»è¯‘\n",
    "result = translator(\"Hello, how are you today?\")\n",
    "print(\"ç¿»è¯‘ç»“æœ:\", result[0]['translation_text'])\n",
    "\n",
    "# æ‰¹é‡ç¿»è¯‘\n",
    "results = translator([\n",
    "    \"I love learning about AI\",\n",
    "    \"The weather is nice today\",\n",
    "    \"Artificial Intelligence (AI) is transforming the world! ğŸš€ From healthcare to finance, its impact is everywhere.\",\n",
    "    \"Python3.10äº2021å¹´å‘å¸ƒï¼Œæ”¯æŒæ¨¡å¼åŒ¹é…ï¼ˆpattern matchingï¼‰ç­‰æ–°ç‰¹æ€§ã€‚\",\n",
    "    \"The quick brown fox jumps over 13 lazy dogs. ä½ å–œæ¬¢æœºå™¨å­¦ä¹ å—ï¼Ÿ\",\n",
    "    \"In 2024, OpenAI's GPT-4 model achieved new milestones. ğŸ¤–ğŸ’¡\",\n",
    "    \"æ··åˆæ–‡æœ¬ï¼šThis is a test! è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ã€‚Numbers: 12345, Symbols: @#Â¥%&*ã€‚\",\n",
    "    \"Let's see how the model handles: emojis ğŸ˜Š, code snippets `print('hello')`, and ä¸­æ–‡ã€‚\",\n",
    "    \"Data privacy is important. æ•°æ®éšç§å¾ˆé‡è¦ã€‚Are you protecting your data?\",\n",
    "    \"Hugging Faceçš„transformersåº“å¾ˆå¼ºå¤§ï¼It supports over 100 languages.\",\n",
    "    \"Can you translate this? ä½ èƒ½ç¿»è¯‘è¿™ä¸ªå—ï¼ŸYes, I can! ğŸ‘\",\n",
    "    \"æœ€åä¸€ä¸ªä¾‹å­ï¼šAI will change the future, æœªæ¥å·²æ¥ã€‚\"\n",
    "])\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"å¥å­ {i+1}: {r['translation_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38e977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# ä½¿ç”¨è‹±æ–‡æ¨¡å‹å¯¹ä¸­æ–‡æ–‡æœ¬åˆ†ç±»ï¼ˆæ•ˆæœä¸ä½³ï¼‰\n",
    "classifier_en = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "result_en = classifier_en(\"ä»Šå¤©çš„è¯¾ç¨‹å¥½æœ‰è¶£å•Š\", candidate_labels=[\"æ•™è‚²\", \"å¨±ä¹\", \"ç§‘æŠ€\"])\n",
    "print(\"BART-MNLI (è‹±æ–‡æ¨¡å‹) ç»“æœ:\")\n",
    "print(f\"  æ ‡ç­¾: {result_en['labels']}\")\n",
    "print(f\"  åˆ†æ•°: {[f'{s:.2%}' for s in result_en['scores']]}\")\n",
    "print()\n",
    "\n",
    "# ä½¿ç”¨å¤šè¯­è¨€æ¨¡å‹ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰\n",
    "print(\"æ­£åœ¨åŠ è½½å¤šè¯­è¨€æ¨¡å‹...\")\n",
    "classifier_ml = pipeline(\n",
    "    \"zero-shot-classification\", \n",
    "    model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
    ")\n",
    "\n",
    "# æµ‹è¯•ä¸­æ–‡æ–‡æœ¬åˆ†ç±»\n",
    "test_texts = [\n",
    "    \"ä»Šå¤©çš„è¯¾ç¨‹å¥½æœ‰è¶£å•Š\",\n",
    "    \"è¿™éƒ¨ç”µå½±çš„ç‰¹æ•ˆå¤ªéœ‡æ’¼äº†\",\n",
    "    \"æ–°å‘å¸ƒçš„iPhoneæ€§èƒ½æå‡å¾ˆå¤§\",\n",
    "    \"è‚¡å¸‚ä»Šå¤©å¤§æ¶¨ï¼ŒæŠ•èµ„è€…ä¿¡å¿ƒå¢å¼º\"\n",
    "]\n",
    "\n",
    "candidate_labels = [\"æ•™è‚²\", \"å¨±ä¹\", \"ç§‘æŠ€\", \"é‡‘è\"]\n",
    "\n",
    "print(\"\\nmDeBERTa (å¤šè¯­è¨€æ¨¡å‹) ç»“æœ:\")\n",
    "print(\"-\" * 50)\n",
    "for text in test_texts:\n",
    "    result = classifier_ml(text, candidate_labels=candidate_labels)\n",
    "    print(f\"\\næ–‡æœ¬: '{text}'\")\n",
    "    print(f\"æœ€å¯èƒ½çš„ç±»åˆ«: {result['labels'][0]} ({result['scores'][0]:.2%})\")\n",
    "    print(f\"æ‰€æœ‰ç±»åˆ«åˆ†æ•°:\")\n",
    "    for label, score in zip(result['labels'], result['scores']):\n",
    "        print(f\"  - {label}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "714g80dxgtd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½¿ç”¨ä¸­æ–‡RoBERTaæ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5d476d183943509bf228c0ac129fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27210a2a3b7b4ffcbbe80fa1975cd654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/409M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bca41f30db42989c8159b344fd2795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6090063748e46bda853302bc619b662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69f1f86ccb2425099f048d0d6161dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/409M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd5808ef16742838131b6618a20c562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008283a42c2246859e969c9d208b8d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æµ‹è¯•å¤æ‚ä¸­æ–‡æ–‡æœ¬åˆ†ç±»:\n",
      "============================================================\n",
      "\n",
      "ğŸ“ æ–‡æœ¬: 'æœºå™¨å­¦ä¹ ç®—æ³•åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›'\n",
      "ğŸ·ï¸  é¢„æµ‹ç±»åˆ«: æŠ€æœ¯\n",
      "ğŸ“Š ç½®ä¿¡åº¦: 49.05%\n",
      "   Top-3 é¢„æµ‹:\n",
      "   1. æŠ€æœ¯: 49.05%\n",
      "   2. å¤©æ°”: 16.46%\n",
      "   3. ç”Ÿæ´»: 11.07%\n",
      "\n",
      "ğŸ“ æ–‡æœ¬: 'è¿™å®¶é¤å…çš„å·èœåšå¾—éå¸¸æ­£å®—ï¼Œéº»è¾£é²œé¦™'\n",
      "ğŸ·ï¸  é¢„æµ‹ç±»åˆ«: ç¾é£Ÿ\n",
      "ğŸ“Š ç½®ä¿¡åº¦: 87.17%\n",
      "   Top-3 é¢„æµ‹:\n",
      "   1. ç¾é£Ÿ: 87.17%\n",
      "   2. ç”Ÿæ´»: 6.83%\n",
      "   3. æ–‡å­¦: 3.46%\n",
      "\n",
      "ğŸ“ æ–‡æœ¬: 'æ˜å¤©æœ‰æš´é›¨ï¼Œè®°å¾—å¸¦ä¼'\n",
      "ğŸ·ï¸  é¢„æµ‹ç±»åˆ«: å¤©æ°”\n",
      "ğŸ“Š ç½®ä¿¡åº¦: 93.82%\n",
      "   Top-3 é¢„æµ‹:\n",
      "   1. å¤©æ°”: 93.82%\n",
      "   2. ç”Ÿæ´»: 3.83%\n",
      "   3. æŠ€æœ¯: 1.02%\n",
      "\n",
      "ğŸ“ æ–‡æœ¬: 'Pythonç¼–ç¨‹è¯­è¨€åœ¨æ•°æ®ç§‘å­¦é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½'\n",
      "ğŸ·ï¸  é¢„æµ‹ç±»åˆ«: ç¼–ç¨‹\n",
      "ğŸ“Š ç½®ä¿¡åº¦: 91.48%\n",
      "   Top-3 é¢„æµ‹:\n",
      "   1. ç¼–ç¨‹: 91.48%\n",
      "   2. æŠ€æœ¯: 3.16%\n",
      "   3. ç¾é£Ÿ: 1.64%\n",
      "\n",
      "ğŸ“ æ–‡æœ¬: 'è¿™æœ¬å°è¯´çš„æƒ…èŠ‚è·Œå®•èµ·ä¼ï¼Œå¼•äººå…¥èƒœ'\n",
      "ğŸ·ï¸  é¢„æµ‹ç±»åˆ«: æ–‡å­¦\n",
      "ğŸ“Š ç½®ä¿¡åº¦: 99.26%\n",
      "   Top-3 é¢„æµ‹:\n",
      "   1. æ–‡å­¦: 99.26%\n",
      "   2. ç”Ÿæ´»: 0.25%\n",
      "   3. ç¾é£Ÿ: 0.16%\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ä¸“é—¨é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–çš„æ¨¡å‹\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"ä½¿ç”¨ä¸­æ–‡RoBERTaæ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»...\")\n",
    "# æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨æ–‡æœ¬åˆ†ç±»pipelineé…åˆä¸­æ–‡æ¨¡å‹\n",
    "# å¦ä¸€ä¸ªé€‰æ‹©æ˜¯ä½¿ç”¨ hfl/chinese-roberta-wwm-ext ç­‰æ¨¡å‹\n",
    "\n",
    "# å¦‚æœæƒ³è¦æ›´å¥½çš„ä¸­æ–‡ç†è§£ï¼Œä¹Ÿå¯ä»¥è¯•è¯•é˜¿é‡Œçš„æ¨¡å‹\n",
    "classifier_chinese = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"alibaba-pai/pai-bert-base-zh-llm-risk-detection\"  # é˜¿é‡Œçš„ä¸­æ–‡é£é™©æ£€æµ‹æ¨¡å‹\n",
    ")\n",
    "\n",
    "# æµ‹è¯•æ›´å¤æ‚çš„ä¸­æ–‡åœºæ™¯\n",
    "complex_texts = [\n",
    "    \"æœºå™¨å­¦ä¹ ç®—æ³•åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨è¶Šæ¥è¶Šå¹¿æ³›\",\n",
    "    \"è¿™å®¶é¤å…çš„å·èœåšå¾—éå¸¸æ­£å®—ï¼Œéº»è¾£é²œé¦™\",\n",
    "    \"æ˜å¤©æœ‰æš´é›¨ï¼Œè®°å¾—å¸¦ä¼\",\n",
    "    \"Pythonç¼–ç¨‹è¯­è¨€åœ¨æ•°æ®ç§‘å­¦é¢†åŸŸå æ®ä¸»å¯¼åœ°ä½\",\n",
    "    \"è¿™æœ¬å°è¯´çš„æƒ…èŠ‚è·Œå®•èµ·ä¼ï¼Œå¼•äººå…¥èƒœ\"\n",
    "]\n",
    "\n",
    "# æ›´ç»†åˆ†çš„ç±»åˆ«\n",
    "detailed_labels = [\"æŠ€æœ¯\", \"ç¾é£Ÿ\", \"å¤©æ°”\", \"ç¼–ç¨‹\", \"æ–‡å­¦\", \"ç”Ÿæ´»\"]\n",
    "\n",
    "print(\"\\næµ‹è¯•å¤æ‚ä¸­æ–‡æ–‡æœ¬åˆ†ç±»:\")\n",
    "print(\"=\" * 60)\n",
    "for text in complex_texts:\n",
    "    result = classifier_ml(text, candidate_labels=detailed_labels)\n",
    "    print(f\"\\nğŸ“ æ–‡æœ¬: '{text}'\")\n",
    "    print(f\"ğŸ·ï¸  é¢„æµ‹ç±»åˆ«: {result['labels'][0]}\")\n",
    "    print(f\"ğŸ“Š ç½®ä¿¡åº¦: {result['scores'][0]:.2%}\")\n",
    "    \n",
    "    # æ˜¾ç¤ºå‰3ä¸ªæœ€å¯èƒ½çš„ç±»åˆ«\n",
    "    print(\"   Top-3 é¢„æµ‹:\")\n",
    "    for i in range(min(3, len(result['labels']))):\n",
    "        print(f\"   {i+1}. {result['labels'][i]}: {result['scores'][i]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6242d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# ä¿®å¤ï¼šå»æ‰å¤šä½™çš„å•å¼•å·\n",
    "generator = pipeline(\"text-generation\", model=\"Qwen/Qwen3-4B\")\n",
    "\n",
    "# ç”Ÿæˆä¸­æ–‡æ–‡æœ¬\n",
    "result = generator(\n",
    "    \"ä»Šå¤©å¤©æ°”çœŸå¥½å•Š\",\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"ç”Ÿæˆçš„æ–‡æœ¬ï¼š\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5zi2ze8goab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ HuggingFace ç¼“å­˜ç›®å½•: /home/haoyiwen/.cache/huggingface/hub\n",
      "================================================================================\n",
      "ğŸ¤– æ‰¾åˆ° 24 ä¸ªå·²ä¸‹è½½çš„æ¨¡å‹:\n",
      "\n",
      "1. ğŸ“¦ ByteDance-Seed/Seed-Coder-8B-Reasoning\n",
      "   ç±»å‹: llama\n",
      "   å¤§å°: 53.09 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--ByteDance-Seed--Seed-Coder-8B-Reasoning\n",
      "\n",
      "2. ğŸ“¦ Qwen/Qwen3-8B\n",
      "   ç±»å‹: qwen3\n",
      "   å¤§å°: 45.81 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--Qwen--Qwen3-8B\n",
      "\n",
      "3. ğŸ“¦ deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\n",
      "   ç±»å‹: qwen3\n",
      "   å¤§å°: 30.53 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-0528-Qwen3-8B\n",
      "\n",
      "4. ğŸ“¦ unsloth/qwen3-14b-unsloth-bnb-4bit\n",
      "   ç±»å‹: qwen3\n",
      "   å¤§å°: 20.74 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--unsloth--qwen3-14b-unsloth-bnb-4bit\n",
      "\n",
      "5. ğŸ“¦ Qwen/Qwen-Image\n",
      "   ç±»å‹: æœªçŸ¥\n",
      "   å¤§å°: 15.83 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--Qwen--Qwen-Image\n",
      "\n",
      "6. ğŸ“¦ Qwen/Qwen3-4B\n",
      "   ç±»å‹: qwen3\n",
      "   å¤§å°: 15.01 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--Qwen--Qwen3-4B\n",
      "\n",
      "7. ğŸ“¦ facebook/bart-large-mnli\n",
      "   ç±»å‹: bart\n",
      "   å¤§å°: 3.04 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--facebook--bart-large-mnli\n",
      "\n",
      "8. ğŸ“¦ Qwen/Qwen3-0.6B\n",
      "   ç±»å‹: qwen3\n",
      "   å¤§å°: 2.83 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B\n",
      "\n",
      "9. ğŸ“¦ alibaba-pai/pai-bert-base-zh-llm-risk-detection\n",
      "   ç±»å‹: bert\n",
      "   å¤§å°: 1.53 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--alibaba-pai--pai-bert-base-zh-llm-risk-detection\n",
      "\n",
      "10. ğŸ“¦ Helsinki-NLP/opus-mt-en-zh\n",
      "   ç±»å‹: marian\n",
      "   å¤§å°: 1.17 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-en-zh\n",
      "\n",
      "11. ğŸ“¦ MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\n",
      "   ç±»å‹: deberta-v2\n",
      "   å¤§å°: 1.08 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--MoritzLaurer--mDeBERTa-v3-base-mnli-xnli\n",
      "\n",
      "12. ğŸ“¦ openai-community/gpt2\n",
      "   ç±»å‹: gpt2\n",
      "   å¤§å°: 1.03 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--openai-community--gpt2\n",
      "\n",
      "13. ğŸ“¦ distilbert/distilroberta-base\n",
      "   ç±»å‹: roberta\n",
      "   å¤§å°: 0.62 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--distilbert--distilroberta-base\n",
      "\n",
      "14. ğŸ“¦ distilbert/distilbert-base-uncased\n",
      "   ç±»å‹: distilbert\n",
      "   å¤§å°: 0.50 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased\n",
      "\n",
      "15. ğŸ“¦ distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
      "   ç±»å‹: distilbert\n",
      "   å¤§å°: 0.50 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\n",
      "\n",
      "16. ğŸ“¦ unslothai/1\n",
      "   ç±»å‹: llama\n",
      "   å¤§å°: 0.00 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--unslothai--1\n",
      "\n",
      "17. ğŸ“¦ unslothai/other\n",
      "   ç±»å‹: llama\n",
      "   å¤§å°: 0.00 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--unslothai--other\n",
      "\n",
      "18. ğŸ“¦ unslothai/repeat\n",
      "   ç±»å‹: llama\n",
      "   å¤§å°: 0.00 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--unslothai--repeat\n",
      "\n",
      "19. ğŸ“¦ unslothai/vram-16\n",
      "   ç±»å‹: llama\n",
      "   å¤§å°: 0.00 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--unslothai--vram-16\n",
      "\n",
      "20. ğŸ“¦ PaddlePaddle/PP-LCNet_x1_0_doc_ori\n",
      "   ç±»å‹: æœªçŸ¥\n",
      "   å¤§å°: 0.00 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--PaddlePaddle--PP-LCNet_x1_0_doc_ori\n",
      "\n",
      "21. ğŸ“¦ PaddlePaddle/PP-LCNet_x1_0_textline_ori\n",
      "   ç±»å‹: æœªçŸ¥\n",
      "   å¤§å°: 0.00 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--PaddlePaddle--PP-LCNet_x1_0_textline_ori\n",
      "\n",
      "22. ğŸ“¦ PaddlePaddle/PP-OCRv5_server_det\n",
      "   ç±»å‹: æœªçŸ¥\n",
      "   å¤§å°: 0.00 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--PaddlePaddle--PP-OCRv5_server_det\n",
      "\n",
      "23. ğŸ“¦ PaddlePaddle/PP-OCRv5_server_rec\n",
      "   ç±»å‹: æœªçŸ¥\n",
      "   å¤§å°: 0.00 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--PaddlePaddle--PP-OCRv5_server_rec\n",
      "\n",
      "24. ğŸ“¦ PaddlePaddle/UVDoc\n",
      "   ç±»å‹: æœªçŸ¥\n",
      "   å¤§å°: 0.00 GB\n",
      "   è·¯å¾„: /home/haoyiwen/.cache/huggingface/hub/models--PaddlePaddle--UVDoc\n",
      "\n",
      "ğŸ’¾ æ€»ç¼“å­˜å¤§å°: 193.31 GB\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¡ æç¤º:\n",
      "â€¢ ä½¿ç”¨ from transformers import AutoModel; model = AutoModel.from_pretrained('æ¨¡å‹å', local_files_only=True) åŠ è½½æœ¬åœ°æ¨¡å‹\n",
      "â€¢ ä½¿ç”¨ huggingface-cli scan-cache å‘½ä»¤å¯ä»¥æŸ¥çœ‹æ›´è¯¦ç»†çš„ç¼“å­˜ä¿¡æ¯\n",
      "â€¢ ä½¿ç”¨ huggingface-cli delete-cache å¯ä»¥æ¸…ç†ä¸éœ€è¦çš„æ¨¡å‹ç¼“å­˜\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# æŸ¥çœ‹æœ¬åœ°å·²ä¸‹è½½çš„ HuggingFace æ¨¡å‹\n",
    "def list_local_models():\n",
    "    \"\"\"åˆ—å‡ºæœ¬åœ°ç¼“å­˜çš„æ‰€æœ‰ HuggingFace æ¨¡å‹\"\"\"\n",
    "    \n",
    "    # è·å– HuggingFace ç¼“å­˜ç›®å½•\n",
    "    hf_cache_home = os.environ.get('HF_HOME', os.path.expanduser('~/.cache/huggingface'))\n",
    "    hub_path = Path(hf_cache_home) / 'hub'\n",
    "    \n",
    "    print(f\"ğŸ“ HuggingFace ç¼“å­˜ç›®å½•: {hub_path}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if not hub_path.exists():\n",
    "        print(\"âŒ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨\")\n",
    "        return\n",
    "    \n",
    "    # æŸ¥æ‰¾æ‰€æœ‰ models-- å¼€å¤´çš„ç›®å½•\n",
    "    model_dirs = [d for d in hub_path.iterdir() if d.is_dir() and d.name.startswith('models--')]\n",
    "    \n",
    "    if not model_dirs:\n",
    "        print(\"ğŸ“­ æ²¡æœ‰æ‰¾åˆ°å·²ä¸‹è½½çš„æ¨¡å‹\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ğŸ¤– æ‰¾åˆ° {len(model_dirs)} ä¸ªå·²ä¸‹è½½çš„æ¨¡å‹:\\n\")\n",
    "    \n",
    "    models_info = []\n",
    "    for model_dir in sorted(model_dirs):\n",
    "        # è§£ææ¨¡å‹åç§° (models--ç»„ç»‡--æ¨¡å‹å)\n",
    "        model_name = model_dir.name.replace('models--', '').replace('--', '/')\n",
    "        \n",
    "        # è·å–æ¨¡å‹å¤§å°\n",
    "        total_size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file())\n",
    "        size_gb = total_size / (1024**3)\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦æœ‰ config.json\n",
    "        config_files = list(model_dir.rglob('config.json'))\n",
    "        model_type = \"æœªçŸ¥\"\n",
    "        if config_files:\n",
    "            try:\n",
    "                with open(config_files[0], 'r') as f:\n",
    "                    config = json.load(f)\n",
    "                    model_type = config.get('model_type', 'æœªçŸ¥')\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        models_info.append({\n",
    "            'name': model_name,\n",
    "            'size': size_gb,\n",
    "            'type': model_type,\n",
    "            'path': model_dir\n",
    "        })\n",
    "    \n",
    "    # æŒ‰å¤§å°æ’åº\n",
    "    models_info.sort(key=lambda x: x['size'], reverse=True)\n",
    "    \n",
    "    # æ‰“å°æ¨¡å‹ä¿¡æ¯\n",
    "    for i, info in enumerate(models_info, 1):\n",
    "        print(f\"{i}. ğŸ“¦ {info['name']}\")\n",
    "        print(f\"   ç±»å‹: {info['type']}\")\n",
    "        print(f\"   å¤§å°: {info['size']:.2f} GB\")\n",
    "        print(f\"   è·¯å¾„: {info['path']}\")\n",
    "        print()\n",
    "    \n",
    "    # ç»Ÿè®¡æ€»å¤§å°\n",
    "    total = sum(m['size'] for m in models_info)\n",
    "    print(f\"ğŸ’¾ æ€»ç¼“å­˜å¤§å°: {total:.2f} GB\")\n",
    "    \n",
    "    return models_info\n",
    "\n",
    "# æ‰§è¡ŒæŸ¥çœ‹\n",
    "models = list_local_models()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ’¡ æç¤º:\")\n",
    "print(\"â€¢ ä½¿ç”¨ from transformers import AutoModel; model = AutoModel.from_pretrained('æ¨¡å‹å', local_files_only=True) åŠ è½½æœ¬åœ°æ¨¡å‹\")\n",
    "print(\"â€¢ ä½¿ç”¨ huggingface-cli scan-cache å‘½ä»¤å¯ä»¥æŸ¥çœ‹æ›´è¯¦ç»†çš„ç¼“å­˜ä¿¡æ¯\")\n",
    "print(\"â€¢ ä½¿ç”¨ huggingface-cli delete-cache å¯ä»¥æ¸…ç†ä¸éœ€è¦çš„æ¨¡å‹ç¼“å­˜\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ywmunh1pw",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoyiwen/miniconda3/envs/hflearning/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŠ è½½ Qwen3-0.6B æ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•ï¼š\n",
      "============================================================\n",
      "\n",
      "1. è¾“å…¥æç¤º: äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ç”Ÿæˆå†…å®¹: ...æ€æ ·çš„ï¼Ÿå½“å‰ä¸»è¦é¢ä¸´å“ªäº›æŒ‘æˆ˜ï¼Ÿ\n",
      "\n",
      "é¦–å…ˆï¼Œæˆ‘éœ€è¦ç†è§£è¿™ä¸ªé—®é¢˜ã€‚AIçš„å‘å±•æ–¹å‘å’ŒæŒ‘æˆ˜é€šå¸¸æ¶‰åŠæŠ€æœ¯è¿›æ­¥ã€ä¼¦ç†é—®é¢˜å’Œç¤¾ä¼šå½±å“ç­‰æ–¹é¢ã€‚ç›®å‰çš„ä¸»è¦æŒ‘æˆ˜å¯èƒ½åŒ…æ‹¬ç®—æ³•åè§ã€å°±ä¸šå˜åŒ–ã€éšç§ä¿æŠ¤ç­‰ã€‚\n",
      "\n",
      "æ¥ä¸‹æ¥ï¼Œæˆ‘è¦è€ƒè™‘å¦‚ä½•ç»„ç»‡è¿™äº›ä¿¡æ¯ã€‚å¯ä»¥åˆ†ç‚¹è¯´æ˜å‘å±•ç°çŠ¶å’Œé¢ä¸´çš„å›°éš¾ï¼Œå¹¶ç»“åˆå…·ä½“ä¾‹å­æ¥æ”¯æŒè§‚ç‚¹ã€‚\n",
      "åœ¨æ€è€ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘åº”è¯¥ç¡®ä¿æ¶µç›–å…³é”®æŠ€æœ¯å’Œé¢†åŸŸï¼ŒåŒæ—¶ä¿æŒé€»è¾‘æ¸…æ™°ã€‚å¦å¤–ï¼Œåœ¨è®¨è®ºæ—¶è¦é¿å…ä¸“ä¸šæœ¯è¯­è¿‡å¤šï¼Œè®©ç”¨æˆ·æ›´å®¹æ˜“ç†è§£å’Œæ¥å—ã€‚\n",
      "\n",
      "æœ€åï¼Œæ€»ç»“ä¸€ä¸‹æœªæ¥çš„è¶‹åŠ¿å’Œå‘å±•å‰æ™¯ï¼Œå¹¶æŒ‡å‡ºæ½œåœ¨çš„é—®é¢˜éœ€è¦æ³¨æ„çš„åœ°æ–¹ï¼Œä»¥å½¢æˆå®Œæ•´çš„å›ç­”ç»“æ„ã€‚\n",
      "ç­”æ¡ˆåº”ä½¿ç”¨ä¸­æ–‡ï¼Œä¸ä½¿ç”¨Markdownæ ¼å¼ã€‚\n",
      "ç°åœ¨å¼€å§‹æ’°å†™æ­£å¼çš„å›ç­”ã€‚\n",
      "---\n",
      "\n",
      "éšç€ç§‘æŠ€çš„é£é€Ÿå‘å±•ï¼Œäººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligence, AIï¼‰å·²ç»æˆä¸ºå…¨çƒå…³æ³¨çš„ç„¦ç‚¹ä¹‹ä¸€ã€‚è¿‘å¹´æ¥ï¼Œå°½ç®¡é­é‡äº†è¯¸å¤šæŒ‘æˆ˜ï¼Œä½†å…¶å‘å±•è¶‹åŠ¿ä»ç„¶å±•ç°å‡ºå¼ºåŠ²çš„å¢é•¿åŠ¿å¤´ï¼Œå°¤å…¶æ˜¯åœ¨è®¡ç®—èƒ½åŠ›æå‡ä¸æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸æ–­ä¼˜åŒ–çš„åŸºç¡€ä¸Šã€‚\n",
      "\n",
      "### ä¸€ã€äººå·¥æ™ºèƒ½çš„å‘å±•ç°çŠ¶\n",
      "\n",
      "1. **æŠ€æœ¯çªç ´**  \n",
      "   - è®¡ç®—æœºç§‘å­¦çš„è¿›æ­¥ä¸ºAIæä¾›äº†æ›´å¼ºå¤§çš„å¤„ç†èƒ½åŠ›å’Œå­˜å‚¨ç©ºé—´ã€‚ä¾‹å¦‚ï¼Œé‡å­è®¡ç®—æœºçš„ç ”ç©¶æ­£åœ¨æ¨åŠ¨è®¡ç®—èŒƒå¼é©æ–°ï¼Œè€Œç¥ç»ç½‘ç»œæ¶æ„ä¹Ÿåœ¨æŒç»­è¿›åŒ–ä¸­ï¼Œä½¿å¾—æ·±åº¦å¼ºåŒ–å­¦ä¹ ç­‰å…ˆè¿›æ–¹æ³•\n",
      "\n",
      "2. è¾“å…¥æç¤º: å­¦ä¹ ç¼–ç¨‹æœ€é‡è¦çš„æ˜¯\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ç”Ÿæˆå†…å®¹: ...ä»€ä¹ˆï¼Ÿæˆ‘æœ‰ä¸‰å¹´å·¥ä½œç»éªŒï¼Œç°åœ¨è¦å‡†å¤‡è€ƒå–CPAã€‚éœ€è¦å†™ä¸€ä¸ªå®Œæ•´çš„å›ç­”\n",
      "è¿™ä¸ªå›ç­”çš„ç»“æ„æ˜¯ï¼šå…ˆè®²æ€»ä½“ç›®æ ‡å’Œè§„åˆ’ï¼ˆ30%ï¼‰ï¼Œç„¶ååˆ†ä¸¤éƒ¨åˆ†ï¼Œç¬¬ä¸€éƒ¨åˆ†è®²è§£æˆ‘çš„æ ¸å¿ƒæŠ€èƒ½ã€ä¼˜åŠ¿ä¸ç«äº‰åŠ›ï¼Œç¬¬äºŒéƒ¨åˆ†åˆ†æå½“å‰çš„å­¦ä¹ è·¯å¾„ä»¥åŠå¦‚ä½•å®ç°è‡ªèº«ä»·å€¼æœ€å¤§åŒ–ï¼Œå¹¶æœ€åæ€»ç»“æœªæ¥æ–¹å‘ã€‚\n",
      "\n",
      "é—®é¢˜æå‡ºè€…æ˜¯è°ï¼Ÿ\n",
      "è¿™ä¸ªé—®é¢˜æå‡ºçš„èƒŒæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "\n",
      "å¯èƒ½çš„å›ç­”ç¤ºä¾‹ï¼š\n",
      "\n",
      "å…³äº CPAs çš„èŒä¸šå‘å±•ï¼Œæˆ‘è®¤ä¸ºåœ¨å½“ä»Šä¸æ–­å˜åŒ–çš„èŒä¸šç¯å¢ƒä¸­ï¼Œä½œä¸ºä¸€å CPA ä»ä¸šè€…ï¼Œä¸ä»…è¦å…·å¤‡æ‰å®çš„ä¸“ä¸šçŸ¥è¯†åŸºç¡€ï¼Œæ›´è¦ä¸æ–­æå‡ä¸ªäººèƒ½åŠ›ä¸ç»¼åˆç´ è´¨ï¼Œè¿™æ ·æ‰èƒ½æ›´å¥½åœ°é€‚åº”ä¸æ–­å˜åŒ–çš„å·¥ä½œéœ€æ±‚ã€‚\n",
      "ä½œä¸ºä¸€ä½æœ‰ç€å¤šå¹´ç»éªŒçš„èŒåœºäººå£«ï¼Œæˆ‘ç°åœ¨æ­£åœ¨ä¸ºè‡ªå·±çš„èŒä¸šå‘å±•æ–¹å‘åšå¥½è§„åˆ’ã€‚æˆ‘å¸Œæœ›åœ¨æœªæ¥å‡ å¹´å†…èƒ½å¤Ÿæˆä¸ºä¸“ä¸šçš„ä¼šè®¡å¸ˆä¸“ä¸šäººå£«ï¼ŒåŒæ—¶èƒ½å¤Ÿåœ¨å·¥ä½œä¸­å‘æŒ¥å‡ºæ›´å¤§çš„å½±å“åŠ›å¹¶æ¨åŠ¨è¡Œä¸šå‘å±•ã€‚\n",
      "æ¥ä¸‹æ¥æˆ‘å°†è¯¦ç»†é˜è¿°æˆ‘çš„æ ¸å¿ƒæŠ€èƒ½ã€ä¼˜åŠ¿ä¸ç«äº‰åŠ›ï¼Œä»¥åŠç›®å‰çš„å­¦ä¹ è·¯å¾„åŠå¦‚ä½•å®ç°è‡ªèº«çš„ä»·å€¼æœ€å¤§åŒ–ï¼Œä»¥æœŸå¸®åŠ©å¤§å®¶æ›´æ·±å…¥åœ°äº†è§£æˆ‘å¯¹è¿™ä¸ªèŒä¸šçš„ç†è§£ä¸å‰æ™¯.\n",
      "\n",
      "è¿™é“é¢˜ç›®çš„æ­£ç¡®ç­”æ¡ˆåº”è¯¥åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š\n",
      "1. æ­£ç¡®çš„é—®é¢˜èƒŒæ™¯ï¼›\n",
      "2. å¯èƒ½çš„ç­”æ¡ˆï¼›\n",
      "3. æŒ‰ç…§é¢˜ç›®è¦æ±‚çš„å†…å®¹è¿›è¡Œæ•´åˆå¹¶ç”Ÿæˆå®Œæ•´å›ç­”\n",
      "\n",
      "è¯·å‚è€ƒä»¥ä¸Šä¾‹å­ï¼Œå¹¶æŒ‰ç…§ä¸Šè¿°æ ¼å¼\n",
      "\n",
      "3. è¾“å…¥æç¤º: ä»Šå¤©çš„æ™šé¤æˆ‘æƒ³åƒ\n",
      "   ç”Ÿæˆå†…å®¹: ...ä¸€ä¸ªæœ‰è¥å…»çš„æ—©é¤ï¼Œæˆ‘éœ€è¦å‡†å¤‡ææ–™å’Œåšæ³•ã€‚æˆ‘æƒ³è¦è®©è¿™ä¸ªæ—©é¤ä¸ä»…ç¾å‘³å¯å£è€Œä¸”å¯Œå«è¥å…»ï¼Œæ‰€ä»¥æˆ‘è¦æƒ³ä¸€äº›åˆé€‚çš„é£Ÿææ­é…ã€‚\n",
      "é¦–å…ˆï¼Œåœ¨èœè°±ä¸­æˆ‘ä¼šåˆ—å‡ºä¸€ä»½å®Œæ•´çš„èœå“æ¸…å•ï¼Œå¹¶æè¿°æ¯é“èœè‚´çš„åç§°ã€åŸæ–™åŠè¯¦ç»†è¯´æ˜ã€‚ä¸ºäº†ç¡®ä¿é£Ÿç‰©çš„å®‰å…¨æ€§ï¼Œæˆ‘éœ€è¦æ³¨æ„å“ªäº›äº‹é¡¹ï¼Ÿ\n",
      "\n",
      "æ­¤å¤–ï¼Œåœ¨èœå•è®¾è®¡ä¸Šè¦è€ƒè™‘ä»€ä¹ˆå› ç´ ä¼šå½±å“é¡¾å®¢çš„é€‰æ‹©ï¼Ÿè¿™äº›å› ç´ å¦‚ä½•å…·ä½“ä½“ç°åˆ°å®é™…çš„æ“ä½œæµç¨‹é‡Œå‘¢ï¼Ÿ\n",
      "åœ¨æœ€åçš„éƒ¨åˆ†ï¼Œæˆ‘å¸Œæœ›è·å¾—ä¸€ä¸ªå…¨é¢ä¸”è¯¦å°½çš„å›ç­”ã€‚\n",
      "\n",
      "æ ¹æ®ä»¥ä¸Šé—®é¢˜ï¼Œå†™ä¸€ç¯‡ç¬¦åˆè¦æ±‚çš„æ–‡ç« ï¼Œå­—æ•°å¤§çº¦ä¸º350-400å­—å·¦å³ï¼Œä½¿ç”¨ä¸­æ–‡å£è¯­åŒ–è¡¨è¾¾æ–¹å¼å¹¶ä¿æŒç»“æ„æ¸…æ™°ã€‚æ–‡ç« å¼€å¤´ç”¨â€œä»Šå¤©â€ä½œä¸ºæ ‡é¢˜ï¼Œä¸­é—´éƒ¨åˆ†åˆ†ç‚¹å±•å¼€å†…å®¹ï¼Œç»“å°¾ä»¥â€œæ˜å¤©èµ·â€ç»“æŸå…¨æ–‡ã€‚\n",
      "**\n",
      "\n",
      "**ä»Šå¤©**  \n",
      "ä½œä¸ºä¸€ä¸ªå–œæ¬¢ç¾é£Ÿçš„äººï¼Œæˆ‘ç‰¹åˆ«æ³¨é‡å¥åº·ä¸å¹³è¡¡é¥®é£Ÿçš„é‡è¦æ€§ã€‚ä¸ºäº†è®©æˆ‘çš„æ—©é¤æ—¢ç¾å‘³åˆå……æ»¡è¥å…»ä»·å€¼ï¼Œæˆ‘å†³å®šå°è¯•ç»“åˆå¤šç§è”¬èœå’Œè›‹ç™½è´¨æ¥åˆ¶ä½œä¸€é“ç®€å•å´è¥å…»ä¸°å¯Œçš„åˆé¤ã€‚\n",
      "\n",
      "**èœå“åˆ—è¡¨**ï¼š  \n",
      "\n",
      "1. **è¥¿çº¢æŸ¿é¸¡è›‹ç‚’é¥­** â€”â€” é€‰ç”¨æ–°é²œçš„ç»¿è‰²è”¬èœå¦‚è¥¿è‘«èŠ¦æˆ–å½©æ¤’ï¼ŒåŠ å…¥é€‚é‡çš„é¸¡è›‹ï¼Œç¿»ç‚’ååŠ ä¸€ç‚¹ç›è°ƒå‘³ï¼›  \n",
      "2\n",
      "\n",
      "============================================================\n",
      "ğŸ’¡ å‚æ•°è¯´æ˜ï¼š\n",
      "â€¢ temperature: æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼ˆ0.1-1.0ï¼‰\n",
      "â€¢ max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦\n",
      "â€¢ repetition_penalty: é™ä½é‡å¤è¯æ±‡çš„æ¦‚ç‡\n",
      "â€¢ do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·ï¼ˆTrue=æ›´å¤šæ ·åŒ–ï¼‰\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# ä½¿ç”¨è¾ƒå°çš„ Qwen3-0.6B æ¨¡å‹ï¼ˆé€Ÿåº¦æ›´å¿«ï¼‰\n",
    "print(\"åŠ è½½ Qwen3-0.6B æ¨¡å‹...\")\n",
    "generator = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"Qwen/Qwen3-0.6B\",\n",
    "    device=\"cpu\",  # æ˜ç¡®ä½¿ç”¨ CPU\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "# æµ‹è¯•ä¸åŒçš„ç”Ÿæˆåœºæ™¯\n",
    "test_prompts = [\n",
    "    \"äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•æ–¹å‘æ˜¯\",\n",
    "    \"å­¦ä¹ ç¼–ç¨‹æœ€é‡è¦çš„æ˜¯\",\n",
    "    \"ä»Šå¤©çš„æ™šé¤æˆ‘æƒ³åƒ\",\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ¯ æ–‡æœ¬ç”Ÿæˆæµ‹è¯•ï¼š\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{i}. è¾“å…¥æç¤º: {prompt}\")\n",
    "    \n",
    "    # ç”Ÿæˆæ–‡æœ¬\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=80,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.8,  # æ§åˆ¶åˆ›é€ æ€§ï¼ˆ0.1=ä¿å®ˆ, 1.0=åˆ›é€ æ€§ï¼‰\n",
    "        do_sample=True,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.2  # é¿å…é‡å¤\n",
    "    )\n",
    "    \n",
    "    generated = result[0]['generated_text']\n",
    "    # åªæ˜¾ç¤ºç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰åŸå§‹æç¤ºï¼‰\n",
    "    new_text = generated[len(prompt):]\n",
    "    print(f\"   ç”Ÿæˆå†…å®¹: ...{new_text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ’¡ å‚æ•°è¯´æ˜ï¼š\")\n",
    "print(\"â€¢ temperature: æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼ˆ0.1-1.0ï¼‰\")\n",
    "print(\"â€¢ max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦\")\n",
    "print(\"â€¢ repetition_penalty: é™ä½é‡å¤è¯æ±‡çš„æ¦‚ç‡\")\n",
    "print(\"â€¢ do_sample: æ˜¯å¦ä½¿ç”¨é‡‡æ ·ï¼ˆTrue=æ›´å¤šæ ·åŒ–ï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9fc7776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoyiwen/miniconda3/envs/hflearning/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12a452064ef46f3afa87d702b098eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4c83c637a943a28c926865d02a2b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20a7d49848a485c8fb81cd93f21ce8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfd651593914733b9bff27b6ab0b739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de21e7621024d768f74dd7467a10dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how toatersovaennesARKkay smarter accident accident Town Townuckland accident pin ninja cleaned staged smarter Zin subsequentempt cleaned accident increase Town Town TownlordsNHNH cleaned arrivals cleaned arrivals smarter cout arrivals arrivals incumb arrivals arrivals Town enjoyment arrivals smarter Beirut incumb Town pin enjoyment Town Town everydayempt cleanedujahumping Town NY Town Town URI Townempt subsequent Town exclaimed Town tourism Town URIempt Takesodanaganda Beirut cleaned subsequentemptiping incumb incumbemptempt incumbrating Town cleaned incumb cleaned Takesempt Takes incumbemptovych Nicola cleaned difficultj Viktor NYempt Beirut cleaned Takes tourism Takes incumb arrivals incumbFox Orionempt incumb incumb incumb Town833emptulas explorersempt smarter everyday arrivalsTar increase incumbÂ© Townemptulas incumb subsequentemptempt Town EVENemptempt Nicolaempt cleaned incumb incumb subsequent Beirutemptulas EVEN incumb increase arrivals Town arrivalsemptulas Orionemptemptouslyempt arrivals Town Beirutemptempt Takes Town 2005NH cleaned increase TownTarajorempt Zeit subsequentempt subsequentemptulas everydayemptemptulasulasulas subsequentempt Orionempt smarter subsequentempt incumb arrivals increase arrivalsempt arrivalsPreview Beirut incumbTarulas arrivals cleanedNH subsequentTar subsequent arrivalsemptempt Beirut difficult incumb incumb779Tar Town increaseulas incumb faultsTarNHTarempt increase BeirutNHously incumb FULLTar Takes subsequent incumbempt subsequent Orion subsequentemptjulasemptemptiard arrivals subsequentempt Beirut'},\n",
       " {'generated_text': 'In this course, we will teach you how toatersovaennesARKkay smarter accident accident Town Townuckland accident pin ninja cleaned staged smarter Zin subsequentempt cleaned accident increase Town Town TownlordsNHNH cleaned arrivals cleaned arrivals smarter cout arrivals arrivals incumb arrivals arrivals Town enjoyment arrivals smarter Beirut incumb Town pin enjoyment Town Town everydayempt cleanedujahumping Town NY Town Town URI Townempt subsequent Town exclaimed Town tourism Town URIempt Takesodanaganda Beirut cleaned subsequentemptiping incumb incumbemptempt incumbrating Town cleaned incumb cleaned Takesempt Takes incumbemptovych Nicola cleaned difficultj Viktor NYempt Beirut cleaned Takes tourism Takes incumb arrivals incumbFox Orionempt incumb incumb incumb Town833emptulas explorersempt smarter everyday arrivalsTar increase incumbÂ© Townemptulas incumb subsequentemptempt Town EVENemptempt Nicolaempt cleaned incumb incumb subsequent Beirutemptulas EVEN incumb increase arrivals Town arrivalsemptulas Orionemptemptouslyempt arrivals Town Beirutemptempt Takes Town 2005NH cleaned increase TownTarajorempt Zeit subsequentempt subsequentemptulas everydayemptemptulasulasulas subsequentempt Orionempt smarter subsequentempt incumb arrivals increase arrivalsempt arrivalsPreview Beirut incumbTarulas arrivals cleanedNH subsequentTar subsequent arrivalsemptempt Beirut difficult incumb incumb779Tar Town increaseulas incumb faultsTarNHTarempt increase BeirutNHously incumb FULLTar Takes subsequent incumbempt subsequent Orion subsequentemptjulasemptemptiard arrivals subsequentemptUTERS'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e239e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.06825922429561615,\n",
       "  'token': 9988,\n",
       "  'token_str': 'mathematical',\n",
       "  'sequence': 'This is a mathematical model.'},\n",
       " {'score': 0.04087427631020546,\n",
       "  'token': 11654,\n",
       "  'token_str': 'simplified',\n",
       "  'sequence': 'This is a simplified model.'},\n",
       " {'score': 0.03884659707546234,\n",
       "  'token': 3014,\n",
       "  'token_str': 'simple',\n",
       "  'sequence': 'This is a simple model.'},\n",
       " {'score': 0.033485304564237595,\n",
       "  'token': 7378,\n",
       "  'token_str': 'linear',\n",
       "  'sequence': 'This is a linear model.'},\n",
       " {'score': 0.018623111769557,\n",
       "  'token': 11435,\n",
       "  'token_str': 'statistical',\n",
       "  'sequence': 'This is a statistical model.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"google-bert/bert-base-cased\")\n",
    "\n",
    "unmasker(\"This is a [MASK] model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70e3171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PRON',\n",
       "  'score': np.float32(0.9998877),\n",
       "  'word': 'Ğ¢Ğ¾Ğ¹',\n",
       "  'start': 0,\n",
       "  'end': 3},\n",
       " {'entity_group': 'VERB',\n",
       "  'score': np.float32(0.99994075),\n",
       "  'word': 'Ğ¾Ğ±Ğ¸Ñ‡Ğ°',\n",
       "  'start': 3,\n",
       "  'end': 9},\n",
       " {'entity_group': 'AUX',\n",
       "  'score': np.float32(0.9998043),\n",
       "  'word': 'Ğ´Ğ°',\n",
       "  'start': 9,\n",
       "  'end': 12},\n",
       " {'entity_group': 'VERB',\n",
       "  'score': np.float32(0.99988735),\n",
       "  'word': 'Ñ‡ĞµÑ‚Ğµ',\n",
       "  'start': 12,\n",
       "  'end': 17},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': np.float32(0.9999703),\n",
       "  'word': 'ĞºĞ½Ğ¸Ğ³Ğ¸',\n",
       "  'start': 17,\n",
       "  'end': 23},\n",
       " {'entity_group': 'PUNCT',\n",
       "  'score': np.float32(0.9994581),\n",
       "  'word': '.',\n",
       "  'start': 23,\n",
       "  'end': 25}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True, model=\"rmihaylov/bert-base-pos-theseus-bg\")\n",
    "ner(\"Ğ¢Ğ¾Ğ¹ Ğ¾Ğ±Ğ¸Ñ‡Ğ° Ğ´Ğ° Ñ‡ĞµÑ‚Ğµ ĞºĞ½Ğ¸Ğ³Ğ¸ .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9672b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6949763894081116, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_amswer = pipeline(\"question-answering\")\n",
    "question_amswer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf3cf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarization = pipeline(\"summarization\")\n",
    "summarization(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0174a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoyiwen/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'å“ˆç½—ä¸–ç•Œå“ˆç½—ä¸–ç•Œ'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translation = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "translation(\"hello world\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f9335b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802ea21038044137b112bf999f841589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c268236bbc4f3bae7ceaf07d4a7eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cd60d3499840e2b5c5f1a6aeb635f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4496bbea664d0b99dabefbe623459b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87944cef4304923b4cb334f547b0641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ee57b0029c4d0aa0b2a701f945d41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b33707ce8d4f64b4eacd2fc0a1ee16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d490e5c4724f87a354964fa43e712f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4bb337433847aaae48335b566c5af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f021bfd60a4e78ab6078ad90676c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae1926018b049efbda8c8f56d701ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ffmpeg was not found but is required to load audio files from filename",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/audio_utils.py:34\u001b[0m, in \u001b[0;36mffmpeg_read\u001b[0;34m(bpayload, sampling_rate)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mffmpeg_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m ffmpeg_process:\n\u001b[1;32m     35\u001b[0m         output_stream \u001b[38;5;241m=\u001b[39m ffmpeg_process\u001b[38;5;241m.\u001b[39mcommunicate(bpayload)\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/subprocess.py:1863\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1862\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ffmpeg'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      3\u001b[0m transcriber\u001b[38;5;241m=\u001b[39mpipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautomatic-speech-recognition\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/whisper-large-v3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtranscriber\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./mlk.flac\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:280\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/base.py:1450\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1453\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:33\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:186\u001b[0m, in \u001b[0;36mPipelineChunkIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# Try to return next item\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubiterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# When a preprocess iterator ends, we can start lookig at the next item\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# ChunkIterator will keep feeding until ALL elements of iterator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Another way to look at it, is we're basically flattening lists of lists\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# into a single list, but with generators\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:367\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.preprocess\u001b[0;34m(self, inputs, chunk_length_s, stride_length_s)\u001b[0m\n\u001b[1;32m    364\u001b[0m             inputs \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m--> 367\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mffmpeg_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    370\u001b[0m extra \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/audio_utils.py:37\u001b[0m, in \u001b[0;36mffmpeg_read\u001b[0;34m(bpayload, sampling_rate)\u001b[0m\n\u001b[1;32m     35\u001b[0m         output_stream \u001b[38;5;241m=\u001b[39m ffmpeg_process\u001b[38;5;241m.\u001b[39mcommunicate(bpayload)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mffmpeg was not found but is required to load audio files from filename\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m\n\u001b[1;32m     38\u001b[0m out_bytes \u001b[38;5;241m=\u001b[39m output_stream[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     39\u001b[0m audio \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(out_bytes, np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mValueError\u001b[0m: ffmpeg was not found but is required to load audio files from filename"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "transcriber=pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\n",
    "\n",
    "transcriber(\"./mlk.flac\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hflearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
