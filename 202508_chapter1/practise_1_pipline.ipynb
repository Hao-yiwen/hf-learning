{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c85a93",
   "metadata": {},
   "source": [
    "# hf中pipline学习示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3357e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch版本: 2.8.0+cu128\n",
      "CUDA是否可用: False\n",
      "\n",
      "=== NVIDIA-SMI信息 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoyiwen/miniconda3/envs/hflearning/lib/python3.10/site-packages/torch/cuda/__init__.py:182: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import subprocess\n",
    "\n",
    "# 检查PyTorch和CUDA版本\n",
    "print(f\"PyTorch版本: {torch.__version__}\")\n",
    "print(f\"CUDA是否可用: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA版本: {torch.version.cuda}\")\n",
    "    print(f\"cuDNN版本: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"GPU数量: {torch.cuda.device_count()}\")\n",
    "    print(f\"当前GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # 获取GPU的compute capability\n",
    "    major, minor = torch.cuda.get_device_capability(0)\n",
    "    print(f\"GPU Compute Capability: {major}.{minor}\")\n",
    "\n",
    "# 用nvidia-smi查看显卡信息\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "    print(\"\\n=== NVIDIA-SMI信息 ===\")\n",
    "    for line in result.stdout.split('\\n')[5:10]:\n",
    "        print(line)\n",
    "except:\n",
    "    print(\"无法运行nvidia-smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ab4589",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoyiwen/miniconda3/envs/hflearning/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.78505939245224}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "classifier(\"I've been waiting for a Hugging Face course my whole life.\")\n",
    "\n",
    "classifier(\"您好\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50c865ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.08184506744146347,\n",
       "  'token': 3143,\n",
       "  'token_str': 'complete',\n",
       "  'sequence': 'this is a complete text'},\n",
       " {'score': 0.07022914290428162,\n",
       "  'token': 7704,\n",
       "  'token_str': 'partial',\n",
       "  'sequence': 'this is a partial text'},\n",
       " {'score': 0.026181718334555626,\n",
       "  'token': 2460,\n",
       "  'token_str': 'short',\n",
       "  'sequence': 'this is a short text'},\n",
       " {'score': 0.019824033603072166,\n",
       "  'token': 3763,\n",
       "  'token_str': 'latin',\n",
       "  'sequence': 'this is a latin text'},\n",
       " {'score': 0.016259148716926575,\n",
       "  'token': 7099,\n",
       "  'token_str': 'sample',\n",
       "  'sequence': 'this is a sample text'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\", model=\"distilbert/distilbert-base-uncased\")\n",
    "unmasker(\"This is a [MASK] text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a04b50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型名称: distilbert/distilbert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "print(\"模型名称:\", unmasker.model.name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc310f82",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unknown task text-to-image, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m textToImage \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-to-image\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mQwen/Qwen-Image\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m textToImage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA beautiful sunset over a calm ocean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/__init__.py:953\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    946\u001b[0m         pipeline_class \u001b[38;5;241m=\u001b[39m get_class_from_dynamic_module(\n\u001b[1;32m    947\u001b[0m             class_ref,\n\u001b[1;32m    948\u001b[0m             model,\n\u001b[1;32m    949\u001b[0m             code_revision\u001b[38;5;241m=\u001b[39mcode_revision,\n\u001b[1;32m    950\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[1;32m    951\u001b[0m         )\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 953\u001b[0m     normalized_task, targeted_task, task_options \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pipeline_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m         pipeline_class \u001b[38;5;241m=\u001b[39m targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpl\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/__init__.py:526\u001b[0m, in \u001b[0;36mcheck_task\u001b[0;34m(task)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_task\u001b[39m(task: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m, Any]:\n\u001b[1;32m    482\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m    Checks an incoming task string, to validate it's correct and return the default Pipeline and Model classes, and\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;124;03m    default models if they exist.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    524\u001b[0m \n\u001b[1;32m    525\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPIPELINE_REGISTRY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/base.py:1540\u001b[0m, in \u001b[0;36mPipelineRegistry.check_task\u001b[0;34m(self, task)\u001b[0m\n\u001b[1;32m   1537\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m task, targeted_task, (tokens[\u001b[38;5;241m1\u001b[39m], tokens[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid translation task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_XX_to_YY\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m format\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1540\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1541\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, available tasks are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_supported_tasks()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_XX_to_YY\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1542\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unknown task text-to-image, available tasks are ['audio-classification', 'automatic-speech-recognition', 'depth-estimation', 'document-question-answering', 'feature-extraction', 'fill-mask', 'image-classification', 'image-feature-extraction', 'image-segmentation', 'image-text-to-text', 'image-to-image', 'image-to-text', 'mask-generation', 'ner', 'object-detection', 'question-answering', 'sentiment-analysis', 'summarization', 'table-question-answering', 'text-classification', 'text-generation', 'text-to-audio', 'text-to-speech', 'text2text-generation', 'token-classification', 'translation', 'video-classification', 'visual-question-answering', 'vqa', 'zero-shot-audio-classification', 'zero-shot-classification', 'zero-shot-image-classification', 'zero-shot-object-detection', 'translation_XX_to_YY']\""
     ]
    }
   ],
   "source": [
    "textToImage = pipeline(\"text-to-image\", mode=\"Qwen/Qwen-Image\")\n",
    "textToImage(\"A beautiful sunset over a calm ocean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6e5b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.8.0+cu128\n",
      "Torchvision: 0.23.0+cu128\n",
      "Transformers: 4.55.4\n",
      "Diffusers: 0.36.0.dev0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import transformers\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Torchvision: {torchvision.__version__}\")\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "\n",
    "# 如果diffusers已安装\n",
    "try:\n",
    "    import diffusers\n",
    "    print(f\"Diffusers: {diffusers.__version__}\")\n",
    "except:\n",
    "    print(\"Diffusers无法导入\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d891629c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "classifier(\n",
    "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd739372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9982948899269104}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\"I've been waiting for a Hugging Face course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a03ffd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoyiwen/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻译结果: 你好,你今天好吗?\n",
      "句子 1: 我喜欢学习AI\n",
      "句子 2: 今天天气天气不错\n",
      "句子 3: 人工智能(AI)正在改变世界!\n",
      "句子 4: Python3.10 082021   (配方匹配)\n",
      "句子 5: 棕色狐狸跳过13只懒狗\n",
      "句子 6: 2024年,OpenAI的GPT-4模型取得了新的里程碑。\n",
      "句子 7: 这是一次测试! 编号:12345,符号:______________________________________________________________________________________________________________________________________________________________________\n",
      "句子 8: 让我们看看模型是如何操作的:mojis {{{{{}},代码片断`print('hello')'和{}}{}}}{}}\n",
      "句子 9: 数据隐私很重要。 您是否保护您的数据 ?\n",
      "句子 10: 它支持超过100种语言。\n",
      "句子 11: 你能翻译吗?\n",
      "句子 12: 将改变未来,\n"
     ]
    }
   ],
   "source": [
    "# 正确的英中翻译方式\n",
    "# 方法1：使用 Helsinki-NLP 的翻译模型\n",
    "from transformers import pipeline\n",
    "\n",
    "# 指定具体的英中翻译模型\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "\n",
    "# 测试翻译\n",
    "result = translator(\"Hello, how are you today?\")\n",
    "print(\"翻译结果:\", result[0]['translation_text'])\n",
    "\n",
    "# 批量翻译\n",
    "results = translator([\n",
    "    \"I love learning about AI\",\n",
    "    \"The weather is nice today\",\n",
    "    \"Artificial Intelligence (AI) is transforming the world! 🚀 From healthcare to finance, its impact is everywhere.\",\n",
    "    \"Python3.10于2021年发布，支持模式匹配（pattern matching）等新特性。\",\n",
    "    \"The quick brown fox jumps over 13 lazy dogs. 你喜欢机器学习吗？\",\n",
    "    \"In 2024, OpenAI's GPT-4 model achieved new milestones. 🤖💡\",\n",
    "    \"混合文本：This is a test! 这是一个测试。Numbers: 12345, Symbols: @#¥%&*。\",\n",
    "    \"Let's see how the model handles: emojis 😊, code snippets `print('hello')`, and 中文。\",\n",
    "    \"Data privacy is important. 数据隐私很重要。Are you protecting your data?\",\n",
    "    \"Hugging Face的transformers库很强大！It supports over 100 languages.\",\n",
    "    \"Can you translate this? 你能翻译这个吗？Yes, I can! 👍\",\n",
    "    \"最后一个例子：AI will change the future, 未来已来。\"\n",
    "])\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"句子 {i+1}: {r['translation_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38e977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 使用英文模型对中文文本分类（效果不佳）\n",
    "classifier_en = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "result_en = classifier_en(\"今天的课程好有趣啊\", candidate_labels=[\"教育\", \"娱乐\", \"科技\"])\n",
    "print(\"BART-MNLI (英文模型) 结果:\")\n",
    "print(f\"  标签: {result_en['labels']}\")\n",
    "print(f\"  分数: {[f'{s:.2%}' for s in result_en['scores']]}\")\n",
    "print()\n",
    "\n",
    "# 使用多语言模型（支持中文）\n",
    "print(\"正在加载多语言模型...\")\n",
    "classifier_ml = pipeline(\n",
    "    \"zero-shot-classification\", \n",
    "    model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"\n",
    ")\n",
    "\n",
    "# 测试中文文本分类\n",
    "test_texts = [\n",
    "    \"今天的课程好有趣啊\",\n",
    "    \"这部电影的特效太震撼了\",\n",
    "    \"新发布的iPhone性能提升很大\",\n",
    "    \"股市今天大涨，投资者信心增强\"\n",
    "]\n",
    "\n",
    "candidate_labels = [\"教育\", \"娱乐\", \"科技\", \"金融\"]\n",
    "\n",
    "print(\"\\nmDeBERTa (多语言模型) 结果:\")\n",
    "print(\"-\" * 50)\n",
    "for text in test_texts:\n",
    "    result = classifier_ml(text, candidate_labels=candidate_labels)\n",
    "    print(f\"\\n文本: '{text}'\")\n",
    "    print(f\"最可能的类别: {result['labels'][0]} ({result['scores'][0]:.2%})\")\n",
    "    print(f\"所有类别分数:\")\n",
    "    for label, score in zip(result['labels'], result['scores']):\n",
    "        print(f\"  - {label}: {score:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "714g80dxgtd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用中文RoBERTa模型进行文本分类...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5d476d183943509bf228c0ac129fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27210a2a3b7b4ffcbbe80fa1975cd654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/409M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bca41f30db42989c8159b344fd2795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6090063748e46bda853302bc619b662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69f1f86ccb2425099f048d0d6161dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/409M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd5808ef16742838131b6618a20c562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008283a42c2246859e969c9d208b8d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试复杂中文文本分类:\n",
      "============================================================\n",
      "\n",
      "📝 文本: '机器学习算法在自然语言处理中的应用越来越广泛'\n",
      "🏷️  预测类别: 技术\n",
      "📊 置信度: 49.05%\n",
      "   Top-3 预测:\n",
      "   1. 技术: 49.05%\n",
      "   2. 天气: 16.46%\n",
      "   3. 生活: 11.07%\n",
      "\n",
      "📝 文本: '这家餐厅的川菜做得非常正宗，麻辣鲜香'\n",
      "🏷️  预测类别: 美食\n",
      "📊 置信度: 87.17%\n",
      "   Top-3 预测:\n",
      "   1. 美食: 87.17%\n",
      "   2. 生活: 6.83%\n",
      "   3. 文学: 3.46%\n",
      "\n",
      "📝 文本: '明天有暴雨，记得带伞'\n",
      "🏷️  预测类别: 天气\n",
      "📊 置信度: 93.82%\n",
      "   Top-3 预测:\n",
      "   1. 天气: 93.82%\n",
      "   2. 生活: 3.83%\n",
      "   3. 技术: 1.02%\n",
      "\n",
      "📝 文本: 'Python编程语言在数据科学领域占据主导地位'\n",
      "🏷️  预测类别: 编程\n",
      "📊 置信度: 91.48%\n",
      "   Top-3 预测:\n",
      "   1. 编程: 91.48%\n",
      "   2. 技术: 3.16%\n",
      "   3. 美食: 1.64%\n",
      "\n",
      "📝 文本: '这本小说的情节跌宕起伏，引人入胜'\n",
      "🏷️  预测类别: 文学\n",
      "📊 置信度: 99.26%\n",
      "   Top-3 预测:\n",
      "   1. 文学: 99.26%\n",
      "   2. 生活: 0.25%\n",
      "   3. 美食: 0.16%\n"
     ]
    }
   ],
   "source": [
    "# 使用专门针对中文优化的模型\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"使用中文RoBERTa模型进行文本分类...\")\n",
    "# 注意：这里使用文本分类pipeline配合中文模型\n",
    "# 另一个选择是使用 hfl/chinese-roberta-wwm-ext 等模型\n",
    "\n",
    "# 如果想要更好的中文理解，也可以试试阿里的模型\n",
    "classifier_chinese = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"alibaba-pai/pai-bert-base-zh-llm-risk-detection\"  # 阿里的中文风险检测模型\n",
    ")\n",
    "\n",
    "# 测试更复杂的中文场景\n",
    "complex_texts = [\n",
    "    \"机器学习算法在自然语言处理中的应用越来越广泛\",\n",
    "    \"这家餐厅的川菜做得非常正宗，麻辣鲜香\",\n",
    "    \"明天有暴雨，记得带伞\",\n",
    "    \"Python编程语言在数据科学领域占据主导地位\",\n",
    "    \"这本小说的情节跌宕起伏，引人入胜\"\n",
    "]\n",
    "\n",
    "# 更细分的类别\n",
    "detailed_labels = [\"技术\", \"美食\", \"天气\", \"编程\", \"文学\", \"生活\"]\n",
    "\n",
    "print(\"\\n测试复杂中文文本分类:\")\n",
    "print(\"=\" * 60)\n",
    "for text in complex_texts:\n",
    "    result = classifier_ml(text, candidate_labels=detailed_labels)\n",
    "    print(f\"\\n📝 文本: '{text}'\")\n",
    "    print(f\"🏷️  预测类别: {result['labels'][0]}\")\n",
    "    print(f\"📊 置信度: {result['scores'][0]:.2%}\")\n",
    "    \n",
    "    # 显示前3个最可能的类别\n",
    "    print(\"   Top-3 预测:\")\n",
    "    for i in range(min(3, len(result['labels']))):\n",
    "        print(f\"   {i+1}. {result['labels'][i]}: {result['scores'][i]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6242d365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 修复：去掉多余的单引号\n",
    "generator = pipeline(\"text-generation\", model=\"Qwen/Qwen3-4B\")\n",
    "\n",
    "# 生成中文文本\n",
    "result = generator(\n",
    "    \"今天天气真好啊\",\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"生成的文本：\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5zi2ze8goab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 HuggingFace 缓存目录: /home/haoyiwen/.cache/huggingface/hub\n",
      "================================================================================\n",
      "🤖 找到 24 个已下载的模型:\n",
      "\n",
      "1. 📦 ByteDance-Seed/Seed-Coder-8B-Reasoning\n",
      "   类型: llama\n",
      "   大小: 53.09 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--ByteDance-Seed--Seed-Coder-8B-Reasoning\n",
      "\n",
      "2. 📦 Qwen/Qwen3-8B\n",
      "   类型: qwen3\n",
      "   大小: 45.81 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--Qwen--Qwen3-8B\n",
      "\n",
      "3. 📦 deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\n",
      "   类型: qwen3\n",
      "   大小: 30.53 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--deepseek-ai--DeepSeek-R1-0528-Qwen3-8B\n",
      "\n",
      "4. 📦 unsloth/qwen3-14b-unsloth-bnb-4bit\n",
      "   类型: qwen3\n",
      "   大小: 20.74 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--unsloth--qwen3-14b-unsloth-bnb-4bit\n",
      "\n",
      "5. 📦 Qwen/Qwen-Image\n",
      "   类型: 未知\n",
      "   大小: 15.83 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--Qwen--Qwen-Image\n",
      "\n",
      "6. 📦 Qwen/Qwen3-4B\n",
      "   类型: qwen3\n",
      "   大小: 15.01 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--Qwen--Qwen3-4B\n",
      "\n",
      "7. 📦 facebook/bart-large-mnli\n",
      "   类型: bart\n",
      "   大小: 3.04 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--facebook--bart-large-mnli\n",
      "\n",
      "8. 📦 Qwen/Qwen3-0.6B\n",
      "   类型: qwen3\n",
      "   大小: 2.83 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B\n",
      "\n",
      "9. 📦 alibaba-pai/pai-bert-base-zh-llm-risk-detection\n",
      "   类型: bert\n",
      "   大小: 1.53 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--alibaba-pai--pai-bert-base-zh-llm-risk-detection\n",
      "\n",
      "10. 📦 Helsinki-NLP/opus-mt-en-zh\n",
      "   类型: marian\n",
      "   大小: 1.17 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--Helsinki-NLP--opus-mt-en-zh\n",
      "\n",
      "11. 📦 MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\n",
      "   类型: deberta-v2\n",
      "   大小: 1.08 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--MoritzLaurer--mDeBERTa-v3-base-mnli-xnli\n",
      "\n",
      "12. 📦 openai-community/gpt2\n",
      "   类型: gpt2\n",
      "   大小: 1.03 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--openai-community--gpt2\n",
      "\n",
      "13. 📦 distilbert/distilroberta-base\n",
      "   类型: roberta\n",
      "   大小: 0.62 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--distilbert--distilroberta-base\n",
      "\n",
      "14. 📦 distilbert/distilbert-base-uncased\n",
      "   类型: distilbert\n",
      "   大小: 0.50 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased\n",
      "\n",
      "15. 📦 distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
      "   类型: distilbert\n",
      "   大小: 0.50 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--distilbert--distilbert-base-uncased-finetuned-sst-2-english\n",
      "\n",
      "16. 📦 unslothai/1\n",
      "   类型: llama\n",
      "   大小: 0.00 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--unslothai--1\n",
      "\n",
      "17. 📦 unslothai/other\n",
      "   类型: llama\n",
      "   大小: 0.00 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--unslothai--other\n",
      "\n",
      "18. 📦 unslothai/repeat\n",
      "   类型: llama\n",
      "   大小: 0.00 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--unslothai--repeat\n",
      "\n",
      "19. 📦 unslothai/vram-16\n",
      "   类型: llama\n",
      "   大小: 0.00 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--unslothai--vram-16\n",
      "\n",
      "20. 📦 PaddlePaddle/PP-LCNet_x1_0_doc_ori\n",
      "   类型: 未知\n",
      "   大小: 0.00 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--PaddlePaddle--PP-LCNet_x1_0_doc_ori\n",
      "\n",
      "21. 📦 PaddlePaddle/PP-LCNet_x1_0_textline_ori\n",
      "   类型: 未知\n",
      "   大小: 0.00 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--PaddlePaddle--PP-LCNet_x1_0_textline_ori\n",
      "\n",
      "22. 📦 PaddlePaddle/PP-OCRv5_server_det\n",
      "   类型: 未知\n",
      "   大小: 0.00 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--PaddlePaddle--PP-OCRv5_server_det\n",
      "\n",
      "23. 📦 PaddlePaddle/PP-OCRv5_server_rec\n",
      "   类型: 未知\n",
      "   大小: 0.00 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--PaddlePaddle--PP-OCRv5_server_rec\n",
      "\n",
      "24. 📦 PaddlePaddle/UVDoc\n",
      "   类型: 未知\n",
      "   大小: 0.00 GB\n",
      "   路径: /home/haoyiwen/.cache/huggingface/hub/models--PaddlePaddle--UVDoc\n",
      "\n",
      "💾 总缓存大小: 193.31 GB\n",
      "\n",
      "================================================================================\n",
      "💡 提示:\n",
      "• 使用 from transformers import AutoModel; model = AutoModel.from_pretrained('模型名', local_files_only=True) 加载本地模型\n",
      "• 使用 huggingface-cli scan-cache 命令可以查看更详细的缓存信息\n",
      "• 使用 huggingface-cli delete-cache 可以清理不需要的模型缓存\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# 查看本地已下载的 HuggingFace 模型\n",
    "def list_local_models():\n",
    "    \"\"\"列出本地缓存的所有 HuggingFace 模型\"\"\"\n",
    "    \n",
    "    # 获取 HuggingFace 缓存目录\n",
    "    hf_cache_home = os.environ.get('HF_HOME', os.path.expanduser('~/.cache/huggingface'))\n",
    "    hub_path = Path(hf_cache_home) / 'hub'\n",
    "    \n",
    "    print(f\"📁 HuggingFace 缓存目录: {hub_path}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if not hub_path.exists():\n",
    "        print(\"❌ 缓存目录不存在\")\n",
    "        return\n",
    "    \n",
    "    # 查找所有 models-- 开头的目录\n",
    "    model_dirs = [d for d in hub_path.iterdir() if d.is_dir() and d.name.startswith('models--')]\n",
    "    \n",
    "    if not model_dirs:\n",
    "        print(\"📭 没有找到已下载的模型\")\n",
    "        return\n",
    "    \n",
    "    print(f\"🤖 找到 {len(model_dirs)} 个已下载的模型:\\n\")\n",
    "    \n",
    "    models_info = []\n",
    "    for model_dir in sorted(model_dirs):\n",
    "        # 解析模型名称 (models--组织--模型名)\n",
    "        model_name = model_dir.name.replace('models--', '').replace('--', '/')\n",
    "        \n",
    "        # 获取模型大小\n",
    "        total_size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file())\n",
    "        size_gb = total_size / (1024**3)\n",
    "        \n",
    "        # 检查是否有 config.json\n",
    "        config_files = list(model_dir.rglob('config.json'))\n",
    "        model_type = \"未知\"\n",
    "        if config_files:\n",
    "            try:\n",
    "                with open(config_files[0], 'r') as f:\n",
    "                    config = json.load(f)\n",
    "                    model_type = config.get('model_type', '未知')\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        models_info.append({\n",
    "            'name': model_name,\n",
    "            'size': size_gb,\n",
    "            'type': model_type,\n",
    "            'path': model_dir\n",
    "        })\n",
    "    \n",
    "    # 按大小排序\n",
    "    models_info.sort(key=lambda x: x['size'], reverse=True)\n",
    "    \n",
    "    # 打印模型信息\n",
    "    for i, info in enumerate(models_info, 1):\n",
    "        print(f\"{i}. 📦 {info['name']}\")\n",
    "        print(f\"   类型: {info['type']}\")\n",
    "        print(f\"   大小: {info['size']:.2f} GB\")\n",
    "        print(f\"   路径: {info['path']}\")\n",
    "        print()\n",
    "    \n",
    "    # 统计总大小\n",
    "    total = sum(m['size'] for m in models_info)\n",
    "    print(f\"💾 总缓存大小: {total:.2f} GB\")\n",
    "    \n",
    "    return models_info\n",
    "\n",
    "# 执行查看\n",
    "models = list_local_models()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"💡 提示:\")\n",
    "print(\"• 使用 from transformers import AutoModel; model = AutoModel.from_pretrained('模型名', local_files_only=True) 加载本地模型\")\n",
    "print(\"• 使用 huggingface-cli scan-cache 命令可以查看更详细的缓存信息\")\n",
    "print(\"• 使用 huggingface-cli delete-cache 可以清理不需要的模型缓存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ywmunh1pw",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoyiwen/miniconda3/envs/hflearning/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载 Qwen3-0.6B 模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 文本生成测试：\n",
      "============================================================\n",
      "\n",
      "1. 输入提示: 人工智能的未来发展方向是\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   生成内容: ...怎样的？当前主要面临哪些挑战？\n",
      "\n",
      "首先，我需要理解这个问题。AI的发展方向和挑战通常涉及技术进步、伦理问题和社会影响等方面。目前的主要挑战可能包括算法偏见、就业变化、隐私保护等。\n",
      "\n",
      "接下来，我要考虑如何组织这些信息。可以分点说明发展现状和面临的困难，并结合具体例子来支持观点。\n",
      "在思考过程中，我应该确保涵盖关键技术和领域，同时保持逻辑清晰。另外，在讨论时要避免专业术语过多，让用户更容易理解和接受。\n",
      "\n",
      "最后，总结一下未来的趋势和发展前景，并指出潜在的问题需要注意的地方，以形成完整的回答结构。\n",
      "答案应使用中文，不使用Markdown格式。\n",
      "现在开始撰写正式的回答。\n",
      "---\n",
      "\n",
      "随着科技的飞速发展，人工智能（Artificial Intelligence, AI）已经成为全球关注的焦点之一。近年来，尽管遭遇了诸多挑战，但其发展趋势仍然展现出强劲的增长势头，尤其是在计算能力提升与深度学习模型不断优化的基础上。\n",
      "\n",
      "### 一、人工智能的发展现状\n",
      "\n",
      "1. **技术突破**  \n",
      "   - 计算机科学的进步为AI提供了更强大的处理能力和存储空间。例如，量子计算机的研究正在推动计算范式革新，而神经网络架构也在持续进化中，使得深度强化学习等先进方法\n",
      "\n",
      "2. 输入提示: 学习编程最重要的是\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   生成内容: ...什么？我有三年工作经验，现在要准备考取CPA。需要写一个完整的回答\n",
      "这个回答的结构是：先讲总体目标和规划（30%），然后分两部分，第一部分讲解我的核心技能、优势与竞争力，第二部分分析当前的学习路径以及如何实现自身价值最大化，并最后总结未来方向。\n",
      "\n",
      "问题提出者是谁？\n",
      "这个问题提出的背景是什么？\n",
      "\n",
      "可能的回答示例：\n",
      "\n",
      "关于 CPAs 的职业发展，我认为在当今不断变化的职业环境中，作为一名 CPA 从业者，不仅要具备扎实的专业知识基础，更要不断提升个人能力与综合素质，这样才能更好地适应不断变化的工作需求。\n",
      "作为一位有着多年经验的职场人士，我现在正在为自己的职业发展方向做好规划。我希望在未来几年内能够成为专业的会计师专业人士，同时能够在工作中发挥出更大的影响力并推动行业发展。\n",
      "接下来我将详细阐述我的核心技能、优势与竞争力，以及目前的学习路径及如何实现自身的价值最大化，以期帮助大家更深入地了解我对这个职业的理解与前景.\n",
      "\n",
      "这道题目的正确答案应该包含以下内容：\n",
      "1. 正确的问题背景；\n",
      "2. 可能的答案；\n",
      "3. 按照题目要求的内容进行整合并生成完整回答\n",
      "\n",
      "请参考以上例子，并按照上述格式\n",
      "\n",
      "3. 输入提示: 今天的晚餐我想吃\n",
      "   生成内容: ...一个有营养的早餐，我需要准备材料和做法。我想要让这个早餐不仅美味可口而且富含营养，所以我要想一些合适的食材搭配。\n",
      "首先，在菜谱中我会列出一份完整的菜品清单，并描述每道菜肴的名称、原料及详细说明。为了确保食物的安全性，我需要注意哪些事项？\n",
      "\n",
      "此外，在菜单设计上要考虑什么因素会影响顾客的选择？这些因素如何具体体现到实际的操作流程里呢？\n",
      "在最后的部分，我希望获得一个全面且详尽的回答。\n",
      "\n",
      "根据以上问题，写一篇符合要求的文章，字数大约为350-400字左右，使用中文口语化表达方式并保持结构清晰。文章开头用“今天”作为标题，中间部分分点展开内容，结尾以“明天起”结束全文。\n",
      "**\n",
      "\n",
      "**今天**  \n",
      "作为一个喜欢美食的人，我特别注重健康与平衡饮食的重要性。为了让我的早餐既美味又充满营养价值，我决定尝试结合多种蔬菜和蛋白质来制作一道简单却营养丰富的午餐。\n",
      "\n",
      "**菜品列表**：  \n",
      "\n",
      "1. **西红柿鸡蛋炒饭** —— 选用新鲜的绿色蔬菜如西葫芦或彩椒，加入适量的鸡蛋，翻炒后加一点盐调味；  \n",
      "2\n",
      "\n",
      "============================================================\n",
      "💡 参数说明：\n",
      "• temperature: 控制输出的随机性（0.1-1.0）\n",
      "• max_length: 最大生成长度\n",
      "• repetition_penalty: 降低重复词汇的概率\n",
      "• do_sample: 是否使用采样（True=更多样化）\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# 使用较小的 Qwen3-0.6B 模型（速度更快）\n",
    "print(\"加载 Qwen3-0.6B 模型...\")\n",
    "generator = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=\"Qwen/Qwen3-0.6B\",\n",
    "    device=\"cpu\",  # 明确使用 CPU\n",
    "    torch_dtype=torch.float32\n",
    ")\n",
    "\n",
    "# 测试不同的生成场景\n",
    "test_prompts = [\n",
    "    \"人工智能的未来发展方向是\",\n",
    "    \"学习编程最重要的是\",\n",
    "    \"今天的晚餐我想吃\",\n",
    "]\n",
    "\n",
    "print(\"\\n🎯 文本生成测试：\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{i}. 输入提示: {prompt}\")\n",
    "    \n",
    "    # 生成文本\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_length=80,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.8,  # 控制创造性（0.1=保守, 1.0=创造性）\n",
    "        do_sample=True,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.2  # 避免重复\n",
    "    )\n",
    "    \n",
    "    generated = result[0]['generated_text']\n",
    "    # 只显示生成的部分（去掉原始提示）\n",
    "    new_text = generated[len(prompt):]\n",
    "    print(f\"   生成内容: ...{new_text}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"💡 参数说明：\")\n",
    "print(\"• temperature: 控制输出的随机性（0.1-1.0）\")\n",
    "print(\"• max_length: 最大生成长度\")\n",
    "print(\"• repetition_penalty: 降低重复词汇的概率\")\n",
    "print(\"• do_sample: 是否使用采样（True=更多样化）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9fc7776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoyiwen/miniconda3/envs/hflearning/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12a452064ef46f3afa87d702b098eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4c83c637a943a28c926865d02a2b17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20a7d49848a485c8fb81cd93f21ce8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfd651593914733b9bff27b6ab0b739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de21e7621024d768f74dd7467a10dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how toatersovaennesARKkay smarter accident accident Town Townuckland accident pin ninja cleaned staged smarter Zin subsequentempt cleaned accident increase Town Town TownlordsNHNH cleaned arrivals cleaned arrivals smarter cout arrivals arrivals incumb arrivals arrivals Town enjoyment arrivals smarter Beirut incumb Town pin enjoyment Town Town everydayempt cleanedujahumping Town NY Town Town URI Townempt subsequent Town exclaimed Town tourism Town URIempt Takesodanaganda Beirut cleaned subsequentemptiping incumb incumbemptempt incumbrating Town cleaned incumb cleaned Takesempt Takes incumbemptovych Nicola cleaned difficultj Viktor NYempt Beirut cleaned Takes tourism Takes incumb arrivals incumbFox Orionempt incumb incumb incumb Town833emptulas explorersempt smarter everyday arrivalsTar increase incumb© Townemptulas incumb subsequentemptempt Town EVENemptempt Nicolaempt cleaned incumb incumb subsequent Beirutemptulas EVEN incumb increase arrivals Town arrivalsemptulas Orionemptemptouslyempt arrivals Town Beirutemptempt Takes Town 2005NH cleaned increase TownTarajorempt Zeit subsequentempt subsequentemptulas everydayemptemptulasulasulas subsequentempt Orionempt smarter subsequentempt incumb arrivals increase arrivalsempt arrivalsPreview Beirut incumbTarulas arrivals cleanedNH subsequentTar subsequent arrivalsemptempt Beirut difficult incumb incumb779Tar Town increaseulas incumb faultsTarNHTarempt increase BeirutNHously incumb FULLTar Takes subsequent incumbempt subsequent Orion subsequentemptjulasemptemptiard arrivals subsequentempt Beirut'},\n",
       " {'generated_text': 'In this course, we will teach you how toatersovaennesARKkay smarter accident accident Town Townuckland accident pin ninja cleaned staged smarter Zin subsequentempt cleaned accident increase Town Town TownlordsNHNH cleaned arrivals cleaned arrivals smarter cout arrivals arrivals incumb arrivals arrivals Town enjoyment arrivals smarter Beirut incumb Town pin enjoyment Town Town everydayempt cleanedujahumping Town NY Town Town URI Townempt subsequent Town exclaimed Town tourism Town URIempt Takesodanaganda Beirut cleaned subsequentemptiping incumb incumbemptempt incumbrating Town cleaned incumb cleaned Takesempt Takes incumbemptovych Nicola cleaned difficultj Viktor NYempt Beirut cleaned Takes tourism Takes incumb arrivals incumbFox Orionempt incumb incumb incumb Town833emptulas explorersempt smarter everyday arrivalsTar increase incumb© Townemptulas incumb subsequentemptempt Town EVENemptempt Nicolaempt cleaned incumb incumb subsequent Beirutemptulas EVEN incumb increase arrivals Town arrivalsemptulas Orionemptemptouslyempt arrivals Town Beirutemptempt Takes Town 2005NH cleaned increase TownTarajorempt Zeit subsequentempt subsequentemptulas everydayemptemptulasulasulas subsequentempt Orionempt smarter subsequentempt incumb arrivals increase arrivalsempt arrivalsPreview Beirut incumbTarulas arrivals cleanedNH subsequentTar subsequent arrivalsemptempt Beirut difficult incumb incumb779Tar Town increaseulas incumb faultsTarNHTarempt increase BeirutNHously incumb FULLTar Takes subsequent incumbempt subsequent Orion subsequentemptjulasemptemptiard arrivals subsequentemptUTERS'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e239e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.06825922429561615,\n",
       "  'token': 9988,\n",
       "  'token_str': 'mathematical',\n",
       "  'sequence': 'This is a mathematical model.'},\n",
       " {'score': 0.04087427631020546,\n",
       "  'token': 11654,\n",
       "  'token_str': 'simplified',\n",
       "  'sequence': 'This is a simplified model.'},\n",
       " {'score': 0.03884659707546234,\n",
       "  'token': 3014,\n",
       "  'token_str': 'simple',\n",
       "  'sequence': 'This is a simple model.'},\n",
       " {'score': 0.033485304564237595,\n",
       "  'token': 7378,\n",
       "  'token_str': 'linear',\n",
       "  'sequence': 'This is a linear model.'},\n",
       " {'score': 0.018623111769557,\n",
       "  'token': 11435,\n",
       "  'token_str': 'statistical',\n",
       "  'sequence': 'This is a statistical model.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"google-bert/bert-base-cased\")\n",
    "\n",
    "unmasker(\"This is a [MASK] model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70e3171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PRON',\n",
       "  'score': np.float32(0.9998877),\n",
       "  'word': 'Той',\n",
       "  'start': 0,\n",
       "  'end': 3},\n",
       " {'entity_group': 'VERB',\n",
       "  'score': np.float32(0.99994075),\n",
       "  'word': 'обича',\n",
       "  'start': 3,\n",
       "  'end': 9},\n",
       " {'entity_group': 'AUX',\n",
       "  'score': np.float32(0.9998043),\n",
       "  'word': 'да',\n",
       "  'start': 9,\n",
       "  'end': 12},\n",
       " {'entity_group': 'VERB',\n",
       "  'score': np.float32(0.99988735),\n",
       "  'word': 'чете',\n",
       "  'start': 12,\n",
       "  'end': 17},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': np.float32(0.9999703),\n",
       "  'word': 'книги',\n",
       "  'start': 17,\n",
       "  'end': 23},\n",
       " {'entity_group': 'PUNCT',\n",
       "  'score': np.float32(0.9994581),\n",
       "  'word': '.',\n",
       "  'start': 23,\n",
       "  'end': 25}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True, model=\"rmihaylov/bert-base-pos-theseus-bg\")\n",
    "ner(\"Той обича да чете книги .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9672b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6949763894081116, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_amswer = pipeline(\"question-answering\")\n",
    "question_amswer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf3cf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarization = pipeline(\"summarization\")\n",
    "summarization(\n",
    "    \"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0174a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haoyiwen/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': '哈罗世界哈罗世界'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translation = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "translation(\"hello world\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f9335b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802ea21038044137b112bf999f841589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c268236bbc4f3bae7ceaf07d4a7eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cd60d3499840e2b5c5f1a6aeb635f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4496bbea664d0b99dabefbe623459b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87944cef4304923b4cb334f547b0641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ee57b0029c4d0aa0b2a701f945d41d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b33707ce8d4f64b4eacd2fc0a1ee16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d490e5c4724f87a354964fa43e712f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d4bb337433847aaae48335b566c5af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f021bfd60a4e78ab6078ad90676c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae1926018b049efbda8c8f56d701ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/340 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ffmpeg was not found but is required to load audio files from filename",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/audio_utils.py:34\u001b[0m, in \u001b[0;36mffmpeg_read\u001b[0;34m(bpayload, sampling_rate)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mffmpeg_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m ffmpeg_process:\n\u001b[1;32m     35\u001b[0m         output_stream \u001b[38;5;241m=\u001b[39m ffmpeg_process\u001b[38;5;241m.\u001b[39mcommunicate(bpayload)\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/subprocess.py:1863\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1862\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ffmpeg'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m      3\u001b[0m transcriber\u001b[38;5;241m=\u001b[39mpipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautomatic-speech-recognition\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/whisper-large-v3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtranscriber\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./mlk.flac\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:280\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/base.py:1450\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1448\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[0;32m-> 1450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1452\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1453\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[1;32m   1454\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[0;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/torch/utils/data/dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:33\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:186\u001b[0m, in \u001b[0;36mPipelineChunkIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# Try to return next item\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubiterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# When a preprocess iterator ends, we can start lookig at the next item\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# ChunkIterator will keep feeding until ALL elements of iterator\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Another way to look at it, is we're basically flattening lists of lists\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# into a single list, but with generators\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py:367\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.preprocess\u001b[0;34m(self, inputs, chunk_length_s, stride_length_s)\u001b[0m\n\u001b[1;32m    364\u001b[0m             inputs \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m--> 367\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mffmpeg_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    370\u001b[0m extra \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/hflearning/lib/python3.10/site-packages/transformers/pipelines/audio_utils.py:37\u001b[0m, in \u001b[0;36mffmpeg_read\u001b[0;34m(bpayload, sampling_rate)\u001b[0m\n\u001b[1;32m     35\u001b[0m         output_stream \u001b[38;5;241m=\u001b[39m ffmpeg_process\u001b[38;5;241m.\u001b[39mcommunicate(bpayload)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mffmpeg was not found but is required to load audio files from filename\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merror\u001b[39;00m\n\u001b[1;32m     38\u001b[0m out_bytes \u001b[38;5;241m=\u001b[39m output_stream[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     39\u001b[0m audio \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(out_bytes, np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[0;31mValueError\u001b[0m: ffmpeg was not found but is required to load audio files from filename"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "transcriber=pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\n",
    "\n",
    "transcriber(\"./mlk.flac\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hflearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
