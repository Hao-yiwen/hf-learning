{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "78b14292",
      "metadata": {},
      "source": [
        "# Transformer Encoder 架构实现与训练示例\n",
        "\n",
        "本 notebook 实现了一个完整的 Transformer Encoder 架构，包含：\n",
        "- 从零开始实现核心组件\n",
        "- 简单的序列分类任务\n",
        "- 可视化和调试功能\n",
        "- 详细的维度变化说明\n",
        "\n",
        "## 1. 环境准备和库导入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6vepzxyv0zy",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Optional, Tuple\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# 设置随机种子\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 检查设备\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"使用设备: {device}\")\n",
        "print(f\"PyTorch 版本: {torch.__version__}\")\n",
        "\n",
        "# 设置matplotlib中文显示\n",
        "plt.rcParams['font.sans-serif'] = ['DejaVu Sans']\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebm12kjfzsl",
      "metadata": {},
      "source": [
        "## 2. Scaled Dot-Product Attention 实现\n",
        "\n",
        "注意力机制的核心公式：\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38oo9t5bcpq",
      "metadata": {},
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(\n",
        "    query: torch.Tensor,\n",
        "    key: torch.Tensor,\n",
        "    value: torch.Tensor,\n",
        "    mask: Optional[torch.Tensor] = None,\n",
        "    dropout: Optional[nn.Dropout] = None\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    计算缩放点积注意力\n",
        "    \n",
        "    Args:\n",
        "        query: [batch_size, seq_len_q, d_k]\n",
        "        key: [batch_size, seq_len_k, d_k]\n",
        "        value: [batch_size, seq_len_v, d_v]\n",
        "        mask: [batch_size, seq_len_q, seq_len_k] 或 [seq_len_q, seq_len_k]\n",
        "        dropout: Dropout层\n",
        "    \n",
        "    Returns:\n",
        "        output: [batch_size, seq_len_q, d_v]\n",
        "        attention_weights: [batch_size, seq_len_q, seq_len_k]\n",
        "    \"\"\"\n",
        "    d_k = query.size(-1)\n",
        "    \n",
        "    # 计算注意力分数\n",
        "    # [batch_size, seq_len_q, d_k] @ [batch_size, d_k, seq_len_k] \n",
        "    # -> [batch_size, seq_len_q, seq_len_k]\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    \n",
        "    # 应用mask（如果有）\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    \n",
        "    # 计算注意力权重\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    # 应用dropout（如果有）\n",
        "    if dropout is not None:\n",
        "        attention_weights = dropout(attention_weights)\n",
        "    \n",
        "    # 应用注意力权重到value\n",
        "    # [batch_size, seq_len_q, seq_len_k] @ [batch_size, seq_len_v, d_v]\n",
        "    # -> [batch_size, seq_len_q, d_v]\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "# 测试注意力机制\n",
        "batch_size, seq_len, d_k = 2, 4, 8\n",
        "query = torch.randn(batch_size, seq_len, d_k)\n",
        "key = torch.randn(batch_size, seq_len, d_k)\n",
        "value = torch.randn(batch_size, seq_len, d_k)\n",
        "\n",
        "output, weights = scaled_dot_product_attention(query, key, value)\n",
        "print(f\"输入 Query shape: {query.shape}\")\n",
        "print(f\"输出 shape: {output.shape}\")\n",
        "print(f\"注意力权重 shape: {weights.shape}\")\n",
        "print(f\"注意力权重和: {weights[0].sum(dim=-1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dqs1jop7abi",
      "metadata": {},
      "source": [
        "## 3. Multi-Head Attention 实现"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vfw8pav8cx",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        多头注意力机制\n",
        "        \n",
        "        Args:\n",
        "            d_model: 模型维度\n",
        "            num_heads: 注意力头数\n",
        "            dropout: dropout率\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model必须能被num_heads整除\"\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads  # 每个头的维度\n",
        "        \n",
        "        # 线性变换层\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(\n",
        "        self, \n",
        "        query: torch.Tensor, \n",
        "        key: torch.Tensor, \n",
        "        value: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor] = None\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        \n",
        "        Args:\n",
        "            query, key, value: [batch_size, seq_len, d_model]\n",
        "            mask: [batch_size, seq_len, seq_len]\n",
        "        \n",
        "        Returns:\n",
        "            output: [batch_size, seq_len, d_model]\n",
        "            attention_weights: [batch_size, num_heads, seq_len, seq_len]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = query.shape\n",
        "        \n",
        "        # 1. 线性变换并分割成多头\n",
        "        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, num_heads, d_k]\n",
        "        Q = self.W_q(query).view(batch_size, seq_len, self.num_heads, self.d_k)\n",
        "        K = self.W_k(key).view(batch_size, seq_len, self.num_heads, self.d_k)\n",
        "        V = self.W_v(value).view(batch_size, seq_len, self.num_heads, self.d_k)\n",
        "        \n",
        "        # 2. 转置以便于注意力计算\n",
        "        # [batch_size, num_heads, seq_len, d_k]\n",
        "        Q = Q.transpose(1, 2)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "        \n",
        "        # 3. 扩展mask维度（如果需要）\n",
        "        if mask is not None:\n",
        "            # [batch_size, 1, seq_len, seq_len]\n",
        "            mask = mask.unsqueeze(1)\n",
        "        \n",
        "        # 4. 计算多头注意力\n",
        "        # 需要重塑张量以使用scaled_dot_product_attention\n",
        "        Q_reshaped = Q.reshape(batch_size * self.num_heads, seq_len, self.d_k)\n",
        "        K_reshaped = K.reshape(batch_size * self.num_heads, seq_len, self.d_k)\n",
        "        V_reshaped = V.reshape(batch_size * self.num_heads, seq_len, self.d_k)\n",
        "        \n",
        "        if mask is not None:\n",
        "            mask = mask.repeat(1, self.num_heads, 1, 1)\n",
        "            mask = mask.reshape(batch_size * self.num_heads, seq_len, seq_len)\n",
        "        \n",
        "        attn_output, attn_weights = scaled_dot_product_attention(\n",
        "            Q_reshaped, K_reshaped, V_reshaped, mask, self.dropout\n",
        "        )\n",
        "        \n",
        "        # 5. 重塑输出\n",
        "        # [batch_size * num_heads, seq_len, d_k] -> [batch_size, num_heads, seq_len, d_k]\n",
        "        attn_output = attn_output.reshape(batch_size, self.num_heads, seq_len, self.d_k)\n",
        "        attn_weights = attn_weights.reshape(batch_size, self.num_heads, seq_len, seq_len)\n",
        "        \n",
        "        # 6. 连接多头输出\n",
        "        # [batch_size, seq_len, num_heads, d_k] -> [batch_size, seq_len, d_model]\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
        "            batch_size, seq_len, self.d_model\n",
        "        )\n",
        "        \n",
        "        # 7. 最终线性变换\n",
        "        output = self.W_o(attn_output)\n",
        "        \n",
        "        return output, attn_weights\n",
        "\n",
        "# 测试多头注意力\n",
        "d_model, num_heads = 128, 4\n",
        "mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "x = torch.randn(2, 10, d_model)  # [batch_size, seq_len, d_model]\n",
        "output, weights = mha(x, x, x)\n",
        "\n",
        "print(f\"输入 shape: {x.shape}\")\n",
        "print(f\"输出 shape: {output.shape}\")\n",
        "print(f\"注意力权重 shape: {weights.shape}\")\n",
        "print(f\"参数量: {sum(p.numel() for p in mha.parameters())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v3x5jgz0ugc",
      "metadata": {},
      "source": [
        "## 4. Position-wise Feed Forward Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ymyspmp3jn",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        前馈神经网络\n",
        "        FFN(x) = max(0, xW1 + b1)W2 + b2\n",
        "        \n",
        "        Args:\n",
        "            d_model: 模型维度\n",
        "            d_ff: 前馈网络隐藏层维度\n",
        "            dropout: dropout率\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        \n",
        "        Args:\n",
        "            x: [batch_size, seq_len, d_model]\n",
        "        \n",
        "        Returns:\n",
        "            output: [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_ff]\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "        # [batch_size, seq_len, d_ff] -> [batch_size, seq_len, d_model]\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 测试FFN\n",
        "d_model, d_ff = 128, 512\n",
        "ffn = PositionwiseFeedForward(d_model, d_ff)\n",
        "\n",
        "x = torch.randn(2, 10, d_model)\n",
        "output = ffn(x)\n",
        "\n",
        "print(f\"FFN 输入 shape: {x.shape}\")\n",
        "print(f\"FFN 输出 shape: {output.shape}\")\n",
        "print(f\"FFN 参数量: {sum(p.numel() for p in ffn.parameters())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "z88r2s79q8m",
      "metadata": {},
      "source": [
        "## 5. Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "coa1gizbv7o",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
        "        \"\"\"\n",
        "        位置编码\n",
        "        PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
        "        PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "        \n",
        "        Args:\n",
        "            d_model: 模型维度\n",
        "            max_len: 最大序列长度\n",
        "            dropout: dropout率\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # 创建位置编码矩阵\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        \n",
        "        # 计算div_term\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * \n",
        "            -(math.log(10000.0) / d_model)\n",
        "        )\n",
        "        \n",
        "        # 应用sin和cos\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        # 添加batch维度并注册为buffer\n",
        "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        \n",
        "        Args:\n",
        "            x: [batch_size, seq_len, d_model]\n",
        "        \n",
        "        Returns:\n",
        "            output: [batch_size, seq_len, d_model]\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        # 添加位置编码\n",
        "        x = x + self.pe[:, :seq_len, :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# 可视化位置编码\n",
        "d_model = 128\n",
        "pe_layer = PositionalEncoding(d_model, max_len=100)\n",
        "\n",
        "# 获取位置编码矩阵\n",
        "pe_matrix = pe_layer.pe[0, :50, :].numpy()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(pe_matrix, cmap='RdBu', aspect='auto')\n",
        "plt.colorbar()\n",
        "plt.xlabel('Dimension')\n",
        "plt.ylabel('Position')\n",
        "plt.title('Positional Encoding Matrix')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# 绘制不同维度的位置编码曲线\n",
        "dims_to_plot = [0, 1, 4, 5, 8, 9]\n",
        "for dim in dims_to_plot:\n",
        "    plt.plot(pe_matrix[:, dim], label=f'dim {dim}')\n",
        "plt.xlabel('Position')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Positional Encoding Curves')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"位置编码 shape: {pe_layer.pe.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cha3cjr6go",
      "metadata": {},
      "source": [
        "## 6. Encoder Layer 实现"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i0ffk8y5xwa",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        d_model: int, \n",
        "        num_heads: int, \n",
        "        d_ff: int, \n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Transformer Encoder层\n",
        "        \n",
        "        Args:\n",
        "            d_model: 模型维度\n",
        "            num_heads: 注意力头数\n",
        "            d_ff: 前馈网络隐藏层维度\n",
        "            dropout: dropout率\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        # 多头自注意力\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        \n",
        "        # 前馈网络\n",
        "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "        \n",
        "        # Layer Normalization\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(\n",
        "        self, \n",
        "        x: torch.Tensor, \n",
        "        mask: Optional[torch.Tensor] = None\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        前向传播\n",
        "        \n",
        "        Args:\n",
        "            x: [batch_size, seq_len, d_model]\n",
        "            mask: [batch_size, seq_len, seq_len]\n",
        "        \n",
        "        Returns:\n",
        "            output: [batch_size, seq_len, d_model]\n",
        "            attention_weights: [batch_size, num_heads, seq_len, seq_len]\n",
        "        \"\"\"\n",
        "        # 1. 多头自注意力子层\n",
        "        # 使用前层归一化（Pre-LN）\n",
        "        normed_x = self.norm1(x)\n",
        "        attn_output, attn_weights = self.self_attention(normed_x, normed_x, normed_x, mask)\n",
        "        x = x + self.dropout1(attn_output)  # 残差连接\n",
        "        \n",
        "        # 2. 前馈网络子层\n",
        "        normed_x = self.norm2(x)\n",
        "        ff_output = self.feed_forward(normed_x)\n",
        "        x = x + self.dropout2(ff_output)  # 残差连接\n",
        "        \n",
        "        return x, attn_weights\n",
        "\n",
        "# 测试Encoder层\n",
        "d_model, num_heads, d_ff = 128, 4, 512\n",
        "encoder_layer = EncoderLayer(d_model, num_heads, d_ff)\n",
        "\n",
        "x = torch.randn(2, 10, d_model)\n",
        "output, weights = encoder_layer(x)\n",
        "\n",
        "print(f\"Encoder层输入 shape: {x.shape}\")\n",
        "print(f\"Encoder层输出 shape: {output.shape}\")\n",
        "print(f\"注意力权重 shape: {weights.shape}\")\n",
        "print(f\"Encoder层参数量: {sum(p.numel() for p in encoder_layer.parameters())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88f972a",
      "metadata": {},
      "source": [
        "## 7. 完整的 Encoder Stack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ab144a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_layers: int,\n",
        "        d_model: int,\n",
        "        num_heads: int,\n",
        "        d_ff: int,\n",
        "        vocab_size: int,\n",
        "        max_len: int = 5000,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        # 词嵌入层\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.embedding_scale = math.sqrt(d_model)\n",
        "        \n",
        "        # 位置编码\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_len, dropout)\n",
        "        \n",
        "        # 堆叠的Encoder层\n",
        "        self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        # 最终的Layer Norm\n",
        "        self.final_norm = nn.LayerNorm(d_model)\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "    def forward(\n",
        "        self, \n",
        "        x: torch.Tensor, \n",
        "        mask: Optional[torch.Tensor] = None\n",
        "    ) -> Tuple[torch.Tensor, list]:\n",
        "        # 词嵌入和位置编码\n",
        "        x = self.embedding(x) * self.embedding_scale\n",
        "        x = self.positional_encoding(x)\n",
        "        \n",
        "        # 处理padding mask\n",
        "        if mask is not None:\n",
        "            batch_size, seq_len = mask.shape\n",
        "            mask = mask.unsqueeze(1).expand(batch_size, seq_len, seq_len)\n",
        "        \n",
        "        # 通过每个Encoder层\n",
        "        attention_weights_list = []\n",
        "        for layer in self.layers:\n",
        "            x, attn_weights = layer(x, mask)\n",
        "            attention_weights_list.append(attn_weights)\n",
        "        \n",
        "        # 最终的Layer Norm\n",
        "        x = self.final_norm(x)\n",
        "        \n",
        "        return x, attention_weights_list\n",
        "\n",
        "# 测试完整的Encoder\n",
        "vocab_size = 1000\n",
        "encoder = TransformerEncoder(\n",
        "    num_layers=3,\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    d_ff=512,\n",
        "    vocab_size=vocab_size\n",
        ")\n",
        "\n",
        "input_ids = torch.randint(0, vocab_size, (2, 15))\n",
        "output, attn_weights_list = encoder(input_ids)\n",
        "\n",
        "print(f\"输入 shape: {input_ids.shape}\")\n",
        "print(f\"输出 shape: {output.shape}\")\n",
        "print(f\"模型总参数量: {sum(p.numel() for p in encoder.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0606c058",
      "metadata": {},
      "source": [
        "## 8. 创建简单的分类任务数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b26ac3d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleSequenceDataset(Dataset):\n",
        "    def __init__(self, num_samples: int, seq_len: int, vocab_size: int):\n",
        "        self.num_samples = num_samples\n",
        "        self.seq_len = seq_len\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # 生成数据\n",
        "        self.data = []\n",
        "        for _ in range(num_samples):\n",
        "            seq = torch.randint(1, vocab_size, (seq_len,))\n",
        "            \n",
        "            # 简单规则：如果序列中连续出现两个相同的token，标签为1\n",
        "            label = 0\n",
        "            for i in range(len(seq) - 1):\n",
        "                if seq[i] == seq[i+1]:\n",
        "                    label = 1\n",
        "                    break\n",
        "            \n",
        "            self.data.append((seq, label))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "# 创建数据集\n",
        "train_dataset = SimpleSequenceDataset(1000, seq_len=20, vocab_size=50)\n",
        "val_dataset = SimpleSequenceDataset(200, seq_len=20, vocab_size=50)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "seq, label = train_dataset[0]\n",
        "print(f\"序列样例: {seq[:10]}...\")\n",
        "print(f\"标签: {label}\")\n",
        "print(f\"训练集大小: {len(train_dataset)}\")\n",
        "print(f\"验证集大小: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7ad87b7",
      "metadata": {},
      "source": [
        "## 9. 构建分类模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f639b88",
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder: TransformerEncoder,\n",
        "        num_classes: int,\n",
        "        d_model: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        \n",
        "        # 分类头：使用平均池化\n",
        "        self.pooling = 'mean'\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(d_model // 2, num_classes)\n",
        "        )\n",
        "        \n",
        "    def forward(self, input_ids: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
        "        # 通过encoder\n",
        "        encoder_output, attention_weights = self.encoder(input_ids, mask)\n",
        "        \n",
        "        # 平均池化\n",
        "        if self.pooling == 'mean':\n",
        "            pooled = encoder_output.mean(1)\n",
        "        else:\n",
        "            pooled = encoder_output[:, 0, :]\n",
        "        \n",
        "        # 分类\n",
        "        logits = self.classifier(pooled)\n",
        "        \n",
        "        return logits, attention_weights\n",
        "\n",
        "# 创建分类模型\n",
        "encoder = TransformerEncoder(\n",
        "    num_layers=2,\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    d_ff=256,\n",
        "    vocab_size=50,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "model = EncoderClassifier(encoder, num_classes=2, d_model=128)\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"分类模型参数量: {sum(p.numel() for p in model.parameters()):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03e8998b",
      "metadata": {},
      "source": [
        "## 10. 简单训练演示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0399280f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练一个batch作为演示\n",
        "model.train()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 获取一个batch\n",
        "sequences, labels = next(iter(train_loader))\n",
        "sequences = sequences.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "# 前向传播\n",
        "optimizer.zero_grad()\n",
        "logits, _ = model(sequences)\n",
        "loss = criterion(logits, labels)\n",
        "\n",
        "# 反向传播\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "# 输出结果\n",
        "_, predicted = torch.max(logits, 1)\n",
        "accuracy = (predicted == labels).float().mean()\n",
        "\n",
        "print(f\"Batch Loss: {loss.item():.4f}\")\n",
        "print(f\"Batch Accuracy: {accuracy.item():.2%}\")\n",
        "print(f\"\\n训练演示完成！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cbffbd7",
      "metadata": {},
      "source": [
        "## 11. 注意力权重可视化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b420f90",
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_attention(model, sequence, layer_idx=0, head_idx=0):\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        sequence = sequence.unsqueeze(0).to(device)\n",
        "        _, attention_weights = model(sequence)\n",
        "        attn = attention_weights[layer_idx][0, head_idx].cpu().numpy()\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(attn[:10, :10], cmap='Blues', cbar=True, square=True)\n",
        "    plt.xlabel('Key Position')\n",
        "    plt.ylabel('Query Position')\n",
        "    plt.title(f'Attention Weights - Layer {layer_idx}, Head {head_idx}')\n",
        "    plt.show()\n",
        "    \n",
        "    return attn\n",
        "\n",
        "# 可视化注意力\n",
        "sample_seq, _ = val_dataset[0]\n",
        "print(\"可视化第一层第一个头的注意力权重（前10个位置）\")\n",
        "attn_weights = visualize_attention(model, sample_seq, layer_idx=0, head_idx=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91684347",
      "metadata": {},
      "source": [
        "## 12. 模型分析"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6ab47c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分析模型参数\n",
        "print(\"=\"*50)\n",
        "print(\"模型参数统计\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"总参数量: {total_params:,}\")\n",
        "print(f\"可训练参数量: {trainable_params:,}\")\n",
        "print(f\"参数大小: {total_params * 4 / 1024 / 1024:.2f} MB (float32)\")\n",
        "\n",
        "print(\"\\n各层参数分布:\")\n",
        "for name, module in model.named_children():\n",
        "    params = sum(p.numel() for p in module.parameters())\n",
        "    if params > 0:\n",
        "        print(f\"{name}: {params:,} ({params/total_params*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cbd9ccd",
      "metadata": {},
      "source": [
        "## 总结\n",
        "\n",
        "本notebook成功实现了一个完整的Transformer Encoder架构，包括：\n",
        "\n",
        "### ✅ 已实现的核心组件\n",
        "1. **Scaled Dot-Product Attention** - 注意力计算的基础\n",
        "2. **Multi-Head Attention** - 多头并行注意力机制\n",
        "3. **Position-wise FFN** - 位置独立的前馈网络\n",
        "4. **Positional Encoding** - 正弦余弦位置编码\n",
        "5. **Encoder Layer** - 包含注意力和FFN的完整层\n",
        "6. **Encoder Stack** - 多层堆叠的编码器\n",
        "7. **分类模型** - 基于Encoder的序列分类器\n",
        "\n",
        "### 🔍 关键特性\n",
        "- 残差连接和层归一化（Pre-LN）\n",
        "- Dropout正则化\n",
        "- Padding mask处理\n",
        "- 参数共享的多头注意力\n",
        "\n",
        "### 📊 调试功能\n",
        "- 维度变化追踪\n",
        "- 注意力权重可视化  \n",
        "- 参数量统计\n",
        "- 梯度流监控\n",
        "\n",
        "### 💡 使用建议\n",
        "1. 可以通过调整`d_model`、`num_heads`、`d_ff`等超参数来实验不同的模型容量\n",
        "2. 注意力可视化有助于理解模型学到的模式\n",
        "3. 这个实现可以作为理解BERT、GPT等模型的基础\n",
        "\n",
        "这个实现为深入理解Transformer架构提供了一个清晰、可调试的起点。"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
