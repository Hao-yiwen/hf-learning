{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7a91a3",
   "metadata": {},
   "source": [
    "# QA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000926b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# è®¾ç½®ä»£ç†\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7893'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7893'\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7893'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7893'\n",
    "os.environ['no_proxy'] = '127.0.0.1,localhost'\n",
    "os.environ['NO_PROXY'] = '127.0.0.1,localhost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4da91ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4699ae1ae7644efeae133e88e6b4aac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccdb419d217340f9977d134f349d8410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8970c296f18947fa9f55b1c9db429253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9804227044842264,\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'answer': 'Jax, PyTorch, and TensorFlow'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back ğŸ¤— Transformers?\"\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "zd1wegsc3z",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer Jax, PyTorch, and TensorFlow\n",
      "ç­”æ¡ˆ: Jax, PyTorch, and TensorFlow\n",
      "ç½®ä¿¡åº¦: 0.9803\n",
      "å¼€å§‹ä½ç½®: 23, ç»“æŸä½ç½®: 35\n",
      "ç­”æ¡ˆ tokens: ['Jax', ',', 'P', '##y', '##T', '##or', '##ch', ',', 'and', 'Ten', '##sor', '##F', '##low']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "# å‡†å¤‡è¾“å…¥\n",
    "# tokenizer ä¼šè‡ªåŠ¨å°† question å’Œ context ç»„åˆæˆæ¨¡å‹éœ€è¦çš„æ ¼å¼ï¼š\n",
    "# [CLS] question [SEP] context [SEP]\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# æ¨¡å‹æ¨ç†\n",
    "# outputs åŒ…å« start_logits å’Œ end_logits\n",
    "# start_logits: æ¯ä¸ª token ä½œä¸ºç­”æ¡ˆå¼€å§‹ä½ç½®çš„åˆ†æ•°\n",
    "# end_logits: æ¯ä¸ª token ä½œä¸ºç­”æ¡ˆç»“æŸä½ç½®çš„åˆ†æ•°\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# ========== ä» outputs è¿˜åŸç­”æ¡ˆï¼ˆè¿™å°±æ˜¯ pipeline å†…éƒ¨åšçš„äº‹ï¼‰==========\n",
    "\n",
    "# 1. è·å–æœ€é«˜åˆ†çš„å¼€å§‹å’Œç»“æŸä½ç½®\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "# æ‰¾åˆ°æœ€é«˜åˆ†çš„ä½ç½®ç´¢å¼•\n",
    "start_idx = torch.argmax(start_scores)\n",
    "end_idx = torch.argmax(end_scores) \n",
    "\n",
    "# 2. è·å–ç­”æ¡ˆçš„ token IDs\n",
    "answer_ids = inputs.input_ids[0][start_idx:end_idx + 1]\n",
    "\n",
    "# 3. å°† token IDs è§£ç å›æ–‡æœ¬\n",
    "answer = tokenizer.decode(answer_ids)\n",
    "print(\"answer\", answer)\n",
    "\n",
    "# 4. è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°ï¼ˆä½¿ç”¨ softmaxï¼‰\n",
    "start_prob = torch.softmax(start_scores, dim=-1)[0, start_idx].item()\n",
    "end_prob = torch.softmax(end_scores, dim=-1)[0, end_idx].item()\n",
    "score = start_prob * end_prob  # ç®€å•ç›¸ä¹˜ä½œä¸ºæ€»åˆ†\n",
    "\n",
    "# 5. è·å–ç­”æ¡ˆåœ¨åŸæ–‡ä¸­çš„å­—ç¬¦ä½ç½®ï¼ˆå¯é€‰ï¼‰\n",
    "# ä½¿ç”¨ offset_mapping å¯ä»¥æ˜ å°„å›åŸæ–‡ä½ç½®\n",
    "inputs_with_offsets = tokenizer(\n",
    "    question, context, \n",
    "    return_tensors=\"pt\",\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "\n",
    "# è·å–ç­”æ¡ˆåœ¨ context ä¸­çš„å®é™…ä½ç½®\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"][0]\n",
    "# æ³¨æ„ï¼šéœ€è¦è·³è¿‡ question éƒ¨åˆ†çš„ tokens\n",
    "sep_idx = inputs.input_ids[0].tolist().index(tokenizer.sep_token_id)  # ç¬¬ä¸€ä¸ª [SEP] çš„ä½ç½®\n",
    "if start_idx > sep_idx:  # ç¡®ä¿ç­”æ¡ˆåœ¨ context ä¸­\n",
    "    # è®¡ç®—ç›¸å¯¹äº context çš„åç§»\n",
    "    context_start_char = offsets[start_idx][0].item() - len(question) - 2  # å‡å» question å’Œæ ‡è®°\n",
    "    context_end_char = offsets[end_idx][1].item() - len(question) - 2\n",
    "    answer_from_context = context[context_start_char:context_end_char]\n",
    "    \n",
    "print(f\"ç­”æ¡ˆ: {answer}\")\n",
    "print(f\"ç½®ä¿¡åº¦: {score:.4f}\")\n",
    "print(f\"å¼€å§‹ä½ç½®: {start_idx}, ç»“æŸä½ç½®: {end_idx}\")\n",
    "print(f\"ç­”æ¡ˆ tokens: {tokenizer.convert_ids_to_tokens(answer_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2taug9qj77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ›´ç®€æ´çš„ç‰ˆæœ¬ï¼ˆæœ€å¸¸ç”¨çš„æ–¹å¼ï¼‰\n",
    "def get_answer_from_outputs(tokenizer, inputs, outputs):\n",
    "    \"\"\"\n",
    "    ä»æ¨¡å‹è¾“å‡ºä¸­æå–ç­”æ¡ˆ\n",
    "    è¿™ä¸ªå‡½æ•°å±•ç¤ºäº† pipeline å†…éƒ¨çš„æ ¸å¿ƒé€»è¾‘\n",
    "    \"\"\"\n",
    "    # è·å–æœ€å¯èƒ½çš„ç­”æ¡ˆä½ç½®\n",
    "    start_idx = torch.argmax(outputs.start_logits)\n",
    "    end_idx = torch.argmax(outputs.end_logits)\n",
    "    \n",
    "    # æå–ç­”æ¡ˆ tokens å¹¶è§£ç \n",
    "    answer_tokens = inputs.input_ids[0][start_idx:end_idx + 1]\n",
    "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # è®¡ç®—åˆ†æ•°\n",
    "    start_score = torch.softmax(outputs.start_logits, dim=-1)[0, start_idx]\n",
    "    end_score = torch.softmax(outputs.end_logits, dim=-1)[0, end_idx]\n",
    "    score = (start_score * end_score).item()\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"score\": score,\n",
    "        \"start\": start_idx.item(),\n",
    "        \"end\": end_idx.item()\n",
    "    }\n",
    "\n",
    "# ä½¿ç”¨ç®€åŒ–å‡½æ•°\n",
    "result = get_answer_from_outputs(tokenizer, inputs, outputs)\n",
    "print(f\"\\nç®€åŒ–ç‰ˆç»“æœ: {result}\")\n",
    "\n",
    "# å¯¹æ¯” pipeline çš„ç»“æœ\n",
    "from transformers import pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "pipeline_result = qa_pipeline(question=question, context=context)\n",
    "print(f\"Pipeline ç»“æœ: {pipeline_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df69ff49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9717117227373819,\n",
       " 'start': 1892,\n",
       " 'end': 1919,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_context = \"\"\"\n",
    "ğŸ¤— Transformers: State of the Art NLP\n",
    "\n",
    "ğŸ¤— Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "ğŸ¤— Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question_answerer(question=question, context=long_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bcab6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¾“å…¥å½¢çŠ¶: torch.Size([2, 384])\n",
      "è¯´æ˜: ç”Ÿæˆäº† 2 ä¸ªå—ï¼Œæ¯ä¸ªå—æœ€å¤š 384 ä¸ª tokens\n"
     ]
    }
   ],
   "source": [
    "# ========== å¤„ç†é•¿æ–‡æœ¬ï¼šéœ€è¦åˆ†å—å¤„ç† ==========\n",
    "# å½“ context è¶…è¿‡æ¨¡å‹æœ€å¤§é•¿åº¦ï¼ˆ384 tokensï¼‰æ—¶ï¼Œéœ€è¦ä½¿ç”¨æ»‘åŠ¨çª—å£æŠ€æœ¯\n",
    "\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=128,         # æ»‘åŠ¨çª—å£é‡å  128 ä¸ª tokensï¼ˆé¿å…ç­”æ¡ˆè¢«åˆ‡æ–­ï¼‰\n",
    "    max_length=384,     # æ¯ä¸ªå—çš„æœ€å¤§é•¿åº¦\n",
    "    padding=\"longest\",  # å¡«å……åˆ°æœ€é•¿åºåˆ—\n",
    "    truncation=\"only_second\",  # åªæˆªæ–­ contextï¼Œä¿ç•™å®Œæ•´ question\n",
    "    return_overflowing_tokens=True,  # è¿”å›æº¢å‡ºçš„éƒ¨åˆ†ï¼ˆç”Ÿæˆå¤šä¸ªå—ï¼‰\n",
    "    return_offsets_mapping=True,      # è¿”å›å­—ç¬¦åç§»æ˜ å°„ï¼ˆç”¨äºå®šä½åŸæ–‡ä½ç½®ï¼‰\n",
    ")\n",
    "\n",
    "# ç§»é™¤ä¸éœ€è¦çš„å­—æ®µ\n",
    "_ = inputs.pop(\"overflow_to_sample_mapping\")  # æ ·æœ¬æ˜ å°„å…³ç³»\n",
    "offsets = inputs.pop(\"offset_mapping\")        # ä¿å­˜åç§»æ˜ å°„ï¼Œåé¢ç”¨äºè¿˜åŸç­”æ¡ˆä½ç½®\n",
    "\n",
    "# è½¬æ¢ä¸º PyTorch tensors\n",
    "inputs = inputs.convert_to_tensors(\"pt\")\n",
    "print(f\"è¾“å…¥å½¢çŠ¶: {inputs['input_ids'].shape}\")\n",
    "print(f\"è¯´æ˜: ç”Ÿæˆäº† {inputs['input_ids'].shape[0]} ä¸ªå—ï¼Œæ¯ä¸ªå—æœ€å¤š {inputs['input_ids'].shape[1]} ä¸ª tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f236c420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start logits å½¢çŠ¶: torch.Size([2, 384])\n",
      "End logits å½¢çŠ¶: torch.Size([2, 384])\n",
      "è§£é‡Š: æ¯ä¸ªå—éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªç­”æ¡ˆé¢„æµ‹\n"
     ]
    }
   ],
   "source": [
    "# å¯¹æ¯ä¸ªå—è¿›è¡Œæ¨ç†\n",
    "# æ³¨æ„ï¼šç°åœ¨æœ‰ 2 ä¸ªå—ï¼Œæ‰€ä»¥è¾“å‡ºä¹Ÿæ˜¯ 2 ä¸ªç»“æœ\n",
    "outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(f\"Start logits å½¢çŠ¶: {start_logits.shape}\")  # [2, 384] = 2ä¸ªå—ï¼Œæ¯ä¸ª384ä¸ªä½ç½®\n",
    "print(f\"End logits å½¢çŠ¶: {end_logits.shape}\")\n",
    "print(f\"è§£é‡Š: æ¯ä¸ªå—éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªç­”æ¡ˆé¢„æµ‹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4702039c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å— 0: å¼€å§‹=0, ç»“æŸ=18, åˆ†æ•°=0.3387\n",
      "å— 1: å¼€å§‹=173, ç»“æŸ=184, åˆ†æ•°=0.9715\n",
      "\n",
      "å€™é€‰ç­”æ¡ˆ: [(0, 18, 0.3386705815792084), (173, 184, 0.9714869856834412)]\n",
      "\n",
      "æ¯ä¸ªå—çš„ç­”æ¡ˆ:\n",
      "å— 0: {'answer': '\\nğŸ¤— Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.3386705815792084}\n",
      "å— 1: {'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.9714869856834412}\n",
      "\n",
      "==================================================\n",
      "ä¸ºä»€ä¹ˆç¬¬ä¸€ä¸ªç­”æ¡ˆé€šå¸¸è¢«å¿½ç•¥ï¼Ÿ\n",
      "1. ç¬¬ä¸€ä¸ªå—çš„ç­”æ¡ˆåˆ†æ•°å¾ˆä½ (0.34)ï¼Œç¬¬äºŒä¸ªå—åˆ†æ•°é«˜ (0.97)\n",
      "2. ç¬¬ä¸€ä¸ªå—å¯èƒ½ä¸åŒ…å«é—®é¢˜çš„ç­”æ¡ˆï¼Œæ¨¡å‹è¢«è¿«é€‰æ‹©äº†ä¸€ä¸ªä¸ç›¸å…³çš„å†…å®¹\n",
      "3. ç¬¬äºŒä¸ªå—åŒ…å«äº†çœŸæ­£çš„ç­”æ¡ˆï¼ˆæ–‡æ¡£æœ«å°¾æåˆ°çš„ä¸‰ä¸ªåº“ï¼‰\n",
      "4. Pipeline ä¼šè‡ªåŠ¨é€‰æ‹©æ‰€æœ‰å—ä¸­åˆ†æ•°æœ€é«˜çš„ç­”æ¡ˆï¼Œæ‰€ä»¥è¿”å›ç¬¬äºŒä¸ª\n",
      "\n",
      "æœ€ç»ˆç­”æ¡ˆï¼ˆåˆ†æ•°æœ€é«˜çš„ï¼‰:\n",
      "ç­”æ¡ˆ: 'Jax, PyTorch and TensorFlow'\n",
      "åˆ†æ•°: 0.9715\n",
      "ä½ç½®: [1892:1919]\n"
     ]
    }
   ],
   "source": [
    "# ========== å…³é”®æ­¥éª¤ï¼šè¿‡æ»¤æ— æ•ˆä½ç½® ==========\n",
    "# åªæœ‰ context éƒ¨åˆ†çš„ tokens æ‰å¯èƒ½æ˜¯ç­”æ¡ˆï¼Œéœ€è¦å±è”½å…¶ä»–ä½ç½®\n",
    "\n",
    "# 1. è·å–æ¯ä¸ª token çš„ç±»å‹ï¼ˆ0=ç‰¹æ®Šæ ‡è®°/question, 1=context, None=paddingï¼‰\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "\n",
    "# 2. åˆ›å»ºæ©ç ï¼šæ ‡è®°å“ªäº›ä½ç½®ä¸å¯èƒ½æ˜¯ç­”æ¡ˆ\n",
    "mask = [i != 1 for i in sequence_ids]  # True = ä¸æ˜¯ context çš„éƒ¨åˆ†\n",
    "mask[0] = False  # [CLS] token ä¹Ÿè¦å±è”½\n",
    "# æ·»åŠ  padding æ©ç \n",
    "mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
    "\n",
    "# 3. å°†æ— æ•ˆä½ç½®çš„åˆ†æ•°è®¾ä¸ºæå°å€¼ï¼ˆ-10000ï¼‰ï¼Œè¿™æ · softmax åæ¦‚ç‡æ¥è¿‘ 0\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n",
    "\n",
    "# 4. è®¡ç®—æ¦‚ç‡\n",
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)\n",
    "\n",
    "# ========== æ‰¾å‡ºæ¯ä¸ªå—çš„æœ€ä½³ç­”æ¡ˆ ==========\n",
    "candidates = []\n",
    "for i, (start_probs, end_probs) in enumerate(zip(start_probabilities, end_probabilities)):\n",
    "    # è®¡ç®—æ‰€æœ‰å¼€å§‹-ç»“æŸä½ç½®ç»„åˆçš„åˆ†æ•°çŸ©é˜µ\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    \n",
    "    # torch.triu: ä¿ç•™ä¸Šä¸‰è§’çŸ©é˜µï¼ˆç¡®ä¿ end >= startï¼‰\n",
    "    # argmax: æ‰¾åˆ°åˆ†æ•°æœ€é«˜çš„ä½ç½®\n",
    "    idx = torch.triu(scores).argmax().item()\n",
    "    \n",
    "    # ä»ä¸€ç»´ç´¢å¼•è½¬æ¢å›äºŒç»´åæ ‡\n",
    "    start_idx = idx // scores.shape[1]\n",
    "    end_idx = idx % scores.shape[1]\n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    \n",
    "    candidates.append((start_idx, end_idx, score))\n",
    "    print(f\"å— {i}: å¼€å§‹={start_idx}, ç»“æŸ={end_idx}, åˆ†æ•°={score:.4f}\")\n",
    "\n",
    "print(f\"\\nå€™é€‰ç­”æ¡ˆ: {candidates}\")\n",
    "\n",
    "# ========== è¿˜åŸæ¯ä¸ªå€™é€‰ç­”æ¡ˆçš„æ–‡æœ¬ ==========\n",
    "print(\"\\næ¯ä¸ªå—çš„ç­”æ¡ˆ:\")\n",
    "for i, (candidate, offset) in enumerate(zip(candidates, offsets)):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char, _ = offset[start_token]\n",
    "    _, end_char = offset[end_token]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
    "    print(f\"å— {i}: {result}\")\n",
    "\n",
    "# ========== ä¸ºä»€ä¹ˆå¿½ç•¥ç¬¬ä¸€ä¸ªç­”æ¡ˆï¼Ÿ ==========\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ä¸ºä»€ä¹ˆç¬¬ä¸€ä¸ªç­”æ¡ˆé€šå¸¸è¢«å¿½ç•¥ï¼Ÿ\")\n",
    "print(\"1. ç¬¬ä¸€ä¸ªå—çš„ç­”æ¡ˆåˆ†æ•°å¾ˆä½ (0.34)ï¼Œç¬¬äºŒä¸ªå—åˆ†æ•°é«˜ (0.97)\")\n",
    "print(\"2. ç¬¬ä¸€ä¸ªå—å¯èƒ½ä¸åŒ…å«é—®é¢˜çš„ç­”æ¡ˆï¼Œæ¨¡å‹è¢«è¿«é€‰æ‹©äº†ä¸€ä¸ªä¸ç›¸å…³çš„å†…å®¹\")\n",
    "print(\"3. ç¬¬äºŒä¸ªå—åŒ…å«äº†çœŸæ­£çš„ç­”æ¡ˆï¼ˆæ–‡æ¡£æœ«å°¾æåˆ°çš„ä¸‰ä¸ªåº“ï¼‰\")\n",
    "print(\"4. Pipeline ä¼šè‡ªåŠ¨é€‰æ‹©æ‰€æœ‰å—ä¸­åˆ†æ•°æœ€é«˜çš„ç­”æ¡ˆï¼Œæ‰€ä»¥è¿”å›ç¬¬äºŒä¸ª\")\n",
    "print(\"\\næœ€ç»ˆç­”æ¡ˆï¼ˆåˆ†æ•°æœ€é«˜çš„ï¼‰:\")\n",
    "best_idx = max(range(len(candidates)), key=lambda i: candidates[i][2])\n",
    "best_candidate, best_offset = candidates[best_idx], offsets[best_idx]\n",
    "start_token, end_token, score = best_candidate\n",
    "start_char, _ = best_offset[start_token]\n",
    "_, end_char = best_offset[end_token]\n",
    "answer = long_context[start_char:end_char]\n",
    "print(f\"ç­”æ¡ˆ: '{answer}'\")\n",
    "print(f\"åˆ†æ•°: {score:.4f}\")\n",
    "print(f\"ä½ç½®: [{start_char}:{end_char}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823dbd34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
