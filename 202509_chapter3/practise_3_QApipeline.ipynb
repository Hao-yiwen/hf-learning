{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7a91a3",
   "metadata": {},
   "source": [
    "# QA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000926b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 设置代理\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7893'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7893'\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7893'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7893'\n",
    "os.environ['no_proxy'] = '127.0.0.1,localhost'\n",
    "os.environ['NO_PROXY'] = '127.0.0.1,localhost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4da91ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4699ae1ae7644efeae133e88e6b4aac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccdb419d217340f9977d134f349d8410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8970c296f18947fa9f55b1c9db429253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 0 files: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9804227044842264,\n",
       " 'start': 78,\n",
       " 'end': 106,\n",
       " 'answer': 'Jax, PyTorch, and TensorFlow'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")\n",
    "context = \"\"\"\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch, and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back 🤗 Transformers?\"\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "zd1wegsc3z",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer Jax, PyTorch, and TensorFlow\n",
      "答案: Jax, PyTorch, and TensorFlow\n",
      "置信度: 0.9803\n",
      "开始位置: 23, 结束位置: 35\n",
      "答案 tokens: ['Jax', ',', 'P', '##y', '##T', '##or', '##ch', ',', 'and', 'Ten', '##sor', '##F', '##low']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 准备输入\n",
    "# tokenizer 会自动将 question 和 context 组合成模型需要的格式：\n",
    "# [CLS] question [SEP] context [SEP]\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "# 模型推理\n",
    "# outputs 包含 start_logits 和 end_logits\n",
    "# start_logits: 每个 token 作为答案开始位置的分数\n",
    "# end_logits: 每个 token 作为答案结束位置的分数\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# ========== 从 outputs 还原答案（这就是 pipeline 内部做的事）==========\n",
    "\n",
    "# 1. 获取最高分的开始和结束位置\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "# 找到最高分的位置索引\n",
    "start_idx = torch.argmax(start_scores)\n",
    "end_idx = torch.argmax(end_scores) \n",
    "\n",
    "# 2. 获取答案的 token IDs\n",
    "answer_ids = inputs.input_ids[0][start_idx:end_idx + 1]\n",
    "\n",
    "# 3. 将 token IDs 解码回文本\n",
    "answer = tokenizer.decode(answer_ids)\n",
    "print(\"answer\", answer)\n",
    "\n",
    "# 4. 计算置信度分数（使用 softmax）\n",
    "start_prob = torch.softmax(start_scores, dim=-1)[0, start_idx].item()\n",
    "end_prob = torch.softmax(end_scores, dim=-1)[0, end_idx].item()\n",
    "score = start_prob * end_prob  # 简单相乘作为总分\n",
    "\n",
    "# 5. 获取答案在原文中的字符位置（可选）\n",
    "# 使用 offset_mapping 可以映射回原文位置\n",
    "inputs_with_offsets = tokenizer(\n",
    "    question, context, \n",
    "    return_tensors=\"pt\",\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "\n",
    "# 获取答案在 context 中的实际位置\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"][0]\n",
    "# 注意：需要跳过 question 部分的 tokens\n",
    "sep_idx = inputs.input_ids[0].tolist().index(tokenizer.sep_token_id)  # 第一个 [SEP] 的位置\n",
    "if start_idx > sep_idx:  # 确保答案在 context 中\n",
    "    # 计算相对于 context 的偏移\n",
    "    context_start_char = offsets[start_idx][0].item() - len(question) - 2  # 减去 question 和标记\n",
    "    context_end_char = offsets[end_idx][1].item() - len(question) - 2\n",
    "    answer_from_context = context[context_start_char:context_end_char]\n",
    "    \n",
    "print(f\"答案: {answer}\")\n",
    "print(f\"置信度: {score:.4f}\")\n",
    "print(f\"开始位置: {start_idx}, 结束位置: {end_idx}\")\n",
    "print(f\"答案 tokens: {tokenizer.convert_ids_to_tokens(answer_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2taug9qj77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更简洁的版本（最常用的方式）\n",
    "def get_answer_from_outputs(tokenizer, inputs, outputs):\n",
    "    \"\"\"\n",
    "    从模型输出中提取答案\n",
    "    这个函数展示了 pipeline 内部的核心逻辑\n",
    "    \"\"\"\n",
    "    # 获取最可能的答案位置\n",
    "    start_idx = torch.argmax(outputs.start_logits)\n",
    "    end_idx = torch.argmax(outputs.end_logits)\n",
    "    \n",
    "    # 提取答案 tokens 并解码\n",
    "    answer_tokens = inputs.input_ids[0][start_idx:end_idx + 1]\n",
    "    answer = tokenizer.decode(answer_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    # 计算分数\n",
    "    start_score = torch.softmax(outputs.start_logits, dim=-1)[0, start_idx]\n",
    "    end_score = torch.softmax(outputs.end_logits, dim=-1)[0, end_idx]\n",
    "    score = (start_score * end_score).item()\n",
    "    \n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"score\": score,\n",
    "        \"start\": start_idx.item(),\n",
    "        \"end\": end_idx.item()\n",
    "    }\n",
    "\n",
    "# 使用简化函数\n",
    "result = get_answer_from_outputs(tokenizer, inputs, outputs)\n",
    "print(f\"\\n简化版结果: {result}\")\n",
    "\n",
    "# 对比 pipeline 的结果\n",
    "from transformers import pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "pipeline_result = qa_pipeline(question=question, context=context)\n",
    "print(f\"Pipeline 结果: {pipeline_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df69ff49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9717117227373819,\n",
       " 'start': 1892,\n",
       " 'end': 1919,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_context = \"\"\"\n",
    "🤗 Transformers: State of the Art NLP\n",
    "\n",
    "🤗 Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,\n",
    "question answering, summarization, translation, text generation and more in over 100 languages.\n",
    "Its aim is to make cutting-edge NLP easier to use for everyone.\n",
    "\n",
    "🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and\n",
    "then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and\n",
    "can be modified to enable quick research experiments.\n",
    "\n",
    "Why should I use transformers?\n",
    "\n",
    "1. Easy-to-use state-of-the-art models:\n",
    "  - High performance on NLU and NLG tasks.\n",
    "  - Low barrier to entry for educators and practitioners.\n",
    "  - Few user-facing abstractions with just three classes to learn.\n",
    "  - A unified API for using all our pretrained models.\n",
    "  - Lower compute costs, smaller carbon footprint:\n",
    "\n",
    "2. Researchers can share trained models instead of always retraining.\n",
    "  - Practitioners can reduce compute time and production costs.\n",
    "  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.\n",
    "\n",
    "3. Choose the right framework for every part of a model's lifetime:\n",
    "  - Train state-of-the-art models in 3 lines of code.\n",
    "  - Move a single model between TF2.0/PyTorch frameworks at will.\n",
    "  - Seamlessly pick the right framework for training, evaluation and production.\n",
    "\n",
    "4. Easily customize a model or an example to your needs:\n",
    "  - We provide examples for each architecture to reproduce the results published by its original authors.\n",
    "  - Model internals are exposed as consistently as possible.\n",
    "  - Model files can be used independently of the library for quick experiments.\n",
    "\n",
    "🤗 Transformers is backed by the three most popular deep learning libraries — Jax, PyTorch and TensorFlow — with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question_answerer(question=question, context=long_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bcab6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入形状: torch.Size([2, 384])\n",
      "说明: 生成了 2 个块，每个块最多 384 个 tokens\n"
     ]
    }
   ],
   "source": [
    "# ========== 处理长文本：需要分块处理 ==========\n",
    "# 当 context 超过模型最大长度（384 tokens）时，需要使用滑动窗口技术\n",
    "\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    long_context,\n",
    "    stride=128,         # 滑动窗口重叠 128 个 tokens（避免答案被切断）\n",
    "    max_length=384,     # 每个块的最大长度\n",
    "    padding=\"longest\",  # 填充到最长序列\n",
    "    truncation=\"only_second\",  # 只截断 context，保留完整 question\n",
    "    return_overflowing_tokens=True,  # 返回溢出的部分（生成多个块）\n",
    "    return_offsets_mapping=True,      # 返回字符偏移映射（用于定位原文位置）\n",
    ")\n",
    "\n",
    "# 移除不需要的字段\n",
    "_ = inputs.pop(\"overflow_to_sample_mapping\")  # 样本映射关系\n",
    "offsets = inputs.pop(\"offset_mapping\")        # 保存偏移映射，后面用于还原答案位置\n",
    "\n",
    "# 转换为 PyTorch tensors\n",
    "inputs = inputs.convert_to_tensors(\"pt\")\n",
    "print(f\"输入形状: {inputs['input_ids'].shape}\")\n",
    "print(f\"说明: 生成了 {inputs['input_ids'].shape[0]} 个块，每个块最多 {inputs['input_ids'].shape[1]} 个 tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f236c420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start logits 形状: torch.Size([2, 384])\n",
      "End logits 形状: torch.Size([2, 384])\n",
      "解释: 每个块都会产生一个答案预测\n"
     ]
    }
   ],
   "source": [
    "# 对每个块进行推理\n",
    "# 注意：现在有 2 个块，所以输出也是 2 个结果\n",
    "outputs = model(**inputs)\n",
    "\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "print(f\"Start logits 形状: {start_logits.shape}\")  # [2, 384] = 2个块，每个384个位置\n",
    "print(f\"End logits 形状: {end_logits.shape}\")\n",
    "print(f\"解释: 每个块都会产生一个答案预测\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4702039c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "块 0: 开始=0, 结束=18, 分数=0.3387\n",
      "块 1: 开始=173, 结束=184, 分数=0.9715\n",
      "\n",
      "候选答案: [(0, 18, 0.3386705815792084), (173, 184, 0.9714869856834412)]\n",
      "\n",
      "每个块的答案:\n",
      "块 0: {'answer': '\\n🤗 Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.3386705815792084}\n",
      "块 1: {'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.9714869856834412}\n",
      "\n",
      "==================================================\n",
      "为什么第一个答案通常被忽略？\n",
      "1. 第一个块的答案分数很低 (0.34)，第二个块分数高 (0.97)\n",
      "2. 第一个块可能不包含问题的答案，模型被迫选择了一个不相关的内容\n",
      "3. 第二个块包含了真正的答案（文档末尾提到的三个库）\n",
      "4. Pipeline 会自动选择所有块中分数最高的答案，所以返回第二个\n",
      "\n",
      "最终答案（分数最高的）:\n",
      "答案: 'Jax, PyTorch and TensorFlow'\n",
      "分数: 0.9715\n",
      "位置: [1892:1919]\n"
     ]
    }
   ],
   "source": [
    "# ========== 关键步骤：过滤无效位置 ==========\n",
    "# 只有 context 部分的 tokens 才可能是答案，需要屏蔽其他位置\n",
    "\n",
    "# 1. 获取每个 token 的类型（0=特殊标记/question, 1=context, None=padding）\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "\n",
    "# 2. 创建掩码：标记哪些位置不可能是答案\n",
    "mask = [i != 1 for i in sequence_ids]  # True = 不是 context 的部分\n",
    "mask[0] = False  # [CLS] token 也要屏蔽\n",
    "# 添加 padding 掩码\n",
    "mask = torch.logical_or(torch.tensor(mask)[None], (inputs[\"attention_mask\"] == 0))\n",
    "\n",
    "# 3. 将无效位置的分数设为极小值（-10000），这样 softmax 后概率接近 0\n",
    "start_logits[mask] = -10000\n",
    "end_logits[mask] = -10000\n",
    "\n",
    "# 4. 计算概率\n",
    "start_probabilities = torch.nn.functional.softmax(start_logits, dim=-1)\n",
    "end_probabilities = torch.nn.functional.softmax(end_logits, dim=-1)\n",
    "\n",
    "# ========== 找出每个块的最佳答案 ==========\n",
    "candidates = []\n",
    "for i, (start_probs, end_probs) in enumerate(zip(start_probabilities, end_probabilities)):\n",
    "    # 计算所有开始-结束位置组合的分数矩阵\n",
    "    scores = start_probs[:, None] * end_probs[None, :]\n",
    "    \n",
    "    # torch.triu: 保留上三角矩阵（确保 end >= start）\n",
    "    # argmax: 找到分数最高的位置\n",
    "    idx = torch.triu(scores).argmax().item()\n",
    "    \n",
    "    # 从一维索引转换回二维坐标\n",
    "    start_idx = idx // scores.shape[1]\n",
    "    end_idx = idx % scores.shape[1]\n",
    "    score = scores[start_idx, end_idx].item()\n",
    "    \n",
    "    candidates.append((start_idx, end_idx, score))\n",
    "    print(f\"块 {i}: 开始={start_idx}, 结束={end_idx}, 分数={score:.4f}\")\n",
    "\n",
    "print(f\"\\n候选答案: {candidates}\")\n",
    "\n",
    "# ========== 还原每个候选答案的文本 ==========\n",
    "print(\"\\n每个块的答案:\")\n",
    "for i, (candidate, offset) in enumerate(zip(candidates, offsets)):\n",
    "    start_token, end_token, score = candidate\n",
    "    start_char, _ = offset[start_token]\n",
    "    _, end_char = offset[end_token]\n",
    "    answer = long_context[start_char:end_char]\n",
    "    result = {\"answer\": answer, \"start\": start_char, \"end\": end_char, \"score\": score}\n",
    "    print(f\"块 {i}: {result}\")\n",
    "\n",
    "# ========== 为什么忽略第一个答案？ ==========\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"为什么第一个答案通常被忽略？\")\n",
    "print(\"1. 第一个块的答案分数很低 (0.34)，第二个块分数高 (0.97)\")\n",
    "print(\"2. 第一个块可能不包含问题的答案，模型被迫选择了一个不相关的内容\")\n",
    "print(\"3. 第二个块包含了真正的答案（文档末尾提到的三个库）\")\n",
    "print(\"4. Pipeline 会自动选择所有块中分数最高的答案，所以返回第二个\")\n",
    "print(\"\\n最终答案（分数最高的）:\")\n",
    "best_idx = max(range(len(candidates)), key=lambda i: candidates[i][2])\n",
    "best_candidate, best_offset = candidates[best_idx], offsets[best_idx]\n",
    "start_token, end_token, score = best_candidate\n",
    "start_char, _ = best_offset[start_token]\n",
    "_, end_char = best_offset[end_token]\n",
    "answer = long_context[start_char:end_char]\n",
    "print(f\"答案: '{answer}'\")\n",
    "print(f\"分数: {score:.4f}\")\n",
    "print(f\"位置: [{start_char}:{end_char}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823dbd34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
