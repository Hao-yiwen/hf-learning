{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7571672c",
   "metadata": {},
   "source": [
    "# Fast tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8647b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 设置代理\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7893'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7893'\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:7893'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:7893'\n",
    "os.environ['no_proxy'] = '127.0.0.1,localhost'\n",
    "os.environ['NO_PROXY'] = '127.0.0.1,localhost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79468bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "encoding = tokenizer(example)\n",
    "print(type(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc187583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5f477d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'S',\n",
       " '##yl',\n",
       " '##va',\n",
       " '##in',\n",
       " 'and',\n",
       " 'I',\n",
       " 'work',\n",
       " 'at',\n",
       " 'Hu',\n",
       " '##gging',\n",
       " 'Face',\n",
       " 'in',\n",
       " 'Brooklyn',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b491ac00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 1, 2, 3, 3, 3, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eebe2e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '81', '##s', '[SEP]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1 = \"81s\"\n",
    "encoding1 = tokenizer(example1)\n",
    "encoding1.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8101b0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 0, 0, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding1.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3c0a4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding.sentence_ids() 方法不存在，使用 token_type_ids 来获取句子ID信息\n",
    "encoding.token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c59f773e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sylvain'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = encoding.word_to_chars(3)\n",
    "example[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45e19f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc88922773e8465684f610149f087f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffbb77b46f944b08343f32ae359c3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de797fe42bd14579a897bdc0f70ab6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072db632f760420f9f226e8367949db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': np.float32(0.99938285),\n",
       "  'index': 4,\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.99815494),\n",
       "  'index': 5,\n",
       "  'word': '##yl',\n",
       "  'start': 12,\n",
       "  'end': 14},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.9959072),\n",
       "  'index': 6,\n",
       "  'word': '##va',\n",
       "  'start': 14,\n",
       "  'end': 16},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.99923277),\n",
       "  'index': 7,\n",
       "  'word': '##in',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9738932),\n",
       "  'index': 12,\n",
       "  'word': 'Hu',\n",
       "  'start': 33,\n",
       "  'end': 35},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.97611505),\n",
       "  'index': 13,\n",
       "  'word': '##gging',\n",
       "  'start': 35,\n",
       "  'end': 40},\n",
       " {'entity': 'I-ORG',\n",
       "  'score': np.float32(0.9887976),\n",
       "  'index': 14,\n",
       "  'word': 'Face',\n",
       "  'start': 41,\n",
       "  'end': 45},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': np.float32(0.9932106),\n",
       "  'index': 16,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53f294b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.9981694),\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.9796019),\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9932106),\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "token_classifier = pipeline(\"token-classification\", aggregation_strategy=\"simple\")\n",
    "token_classifier(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fdff09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint)\n",
    "\n",
    "example = \"My name is Sylvain and I work at Hugging Face in Brooklyn.\"\n",
    "inputs = tokenizer(example, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "hgegp09p9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "手动处理的结果:\n",
      "Entity: I-PER      Word: S               Score: 0.9994\n",
      "Entity: I-PER      Word: ##yl            Score: 0.9982\n",
      "Entity: I-PER      Word: ##va            Score: 0.9959\n",
      "Entity: I-PER      Word: ##in            Score: 0.9992\n",
      "Entity: I-ORG      Word: Hu              Score: 0.9739\n",
      "Entity: I-ORG      Word: ##gging         Score: 0.9761\n",
      "Entity: I-ORG      Word: Face            Score: 0.9888\n",
      "Entity: I-LOC      Word: Brooklyn        Score: 0.9932\n",
      "\n",
      "对比 Pipeline 的结果:\n",
      "Entity: I-PER      Word: S               Score: 0.9994\n",
      "Entity: I-PER      Word: ##yl            Score: 0.9982\n",
      "Entity: I-PER      Word: ##va            Score: 0.9959\n",
      "Entity: I-PER      Word: ##in            Score: 0.9992\n",
      "Entity: I-ORG      Word: Hu              Score: 0.9739\n",
      "Entity: I-ORG      Word: ##gging         Score: 0.9761\n",
      "Entity: I-ORG      Word: Face            Score: 0.9888\n",
      "Entity: I-LOC      Word: Brooklyn        Score: 0.9932\n"
     ]
    }
   ],
   "source": [
    "# 从 logits 转换为 pipeline 格式的结果\n",
    "import torch\n",
    "\n",
    "# 1. 获取预测的标签\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "# 2. 获取标签映射\n",
    "label_list = model.config.id2label\n",
    "\n",
    "# 3. 获取 tokens\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# 4. 获取 softmax 概率\n",
    "probabilities = torch.softmax(outputs.logits[0], dim=-1)\n",
    "\n",
    "# 5. 构建结果（类似 pipeline 输出）\n",
    "results = []\n",
    "for idx, (token, label_id) in enumerate(zip(tokens, predictions[0])):\n",
    "    if token not in [\"[CLS]\", \"[SEP]\"]:  # 过滤特殊标记\n",
    "        label = label_list[label_id.item()]\n",
    "        if label != \"O\":  # 只保留非 O 标签\n",
    "            results.append({\n",
    "                \"entity\": label,\n",
    "                \"score\": probabilities[idx, label_id].item(),\n",
    "                \"index\": idx,\n",
    "                \"word\": token,\n",
    "                \"start\": tokenizer.encode_plus(example, return_offsets_mapping=True)[\"offset_mapping\"][idx][0],\n",
    "                \"end\": tokenizer.encode_plus(example, return_offsets_mapping=True)[\"offset_mapping\"][idx][1]\n",
    "            })\n",
    "\n",
    "print(\"手动处理的结果:\")\n",
    "for item in results:\n",
    "    print(f\"Entity: {item['entity']:10} Word: {item['word']:15} Score: {item['score']:.4f}\")\n",
    "\n",
    "print(\"\\n对比 Pipeline 的结果:\")\n",
    "from transformers import pipeline\n",
    "pipe = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, device=0)\n",
    "pipeline_results = pipe(example)\n",
    "for item in pipeline_results:\n",
    "    print(f\"Entity: {item['entity']:10} Word: {item['word']:15} Score: {item['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ndf0l6yiz7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 重新获取预测和概率\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)[0]  # 获取第一个样本的预测\n",
    "probabilities = torch.softmax(outputs.logits[0], dim=-1)  # 获取概率\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx].item()  # 转换为 Python int\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # Remove the B- or I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx].item()] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][predictions[idx]].item())  # 使用当前位置的预测标签\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "        score = np.mean(all_scores).item() if all_scores else probabilities[idx-1][pred].item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        idx += 1  # 重要：当标签是 \"O\" 时也要增加索引\n",
    "\n",
    "print(\"聚合后的结果（类似 aggregation_strategy='simple'）:\")\n",
    "for r in results:\n",
    "    print(f\"Entity: {r['entity_group']:10} Word: {r['word']:15} Score: {r['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toammn7xrzm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始代码的修正版本\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 必须先定义 predictions 和 probabilities\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)[0]  # 获取预测标签\n",
    "probabilities = torch.softmax(outputs.logits[0], dim=-1)  # 获取概率分布\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx].item()  # 修复：转换为 Python int\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # Remove the B- or I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx].item()] == f\"I-{label}\"  # 修复：加 .item()\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][predictions[idx]].item())  # 修复：使用当前位置的预测\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        idx += 1  # 修复：只在标签是 \"O\" 时才增加（避免重复增加）\n",
    "\n",
    "print(\"修正后的聚合结果:\")\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1wmby6f1ux5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions type: <class 'torch.Tensor'>\n",
      "predictions: tensor([0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0])\n",
      "predictions[0]: 0, type: <class 'torch.Tensor'>\n",
      "\n",
      "聚合后的结果:\n",
      "Entity: PER        Word: 'Sylvain        ' Score: 0.9982 [11:18]\n",
      "Entity: ORG        Word: 'Hugging Face   ' Score: 0.9796 [33:45]\n",
      "Entity: LOC        Word: 'Brooklyn       ' Score: 0.9932 [49:57]\n"
     ]
    }
   ],
   "source": [
    "# 完整修复版本 - 解决 KeyError 问题\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 从 logits 获取预测和概率\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)[0]  # shape: [seq_len]\n",
    "probabilities = torch.softmax(outputs.logits[0], dim=-1)  # shape: [seq_len, num_labels]\n",
    "\n",
    "# 调试：查看 predictions 的类型和内容\n",
    "print(f\"predictions type: {type(predictions)}\")\n",
    "print(f\"predictions: {predictions}\")\n",
    "print(f\"predictions[0]: {predictions[0]}, type: {type(predictions[0])}\")\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    # 关键修复：将 tensor 转换为 int\n",
    "    pred = predictions[idx].item() if torch.is_tensor(predictions[idx]) else predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    \n",
    "    if label != \"O\":\n",
    "        # Remove the B- or I- prefix\n",
    "        label = label[2:] if label.startswith((\"B-\", \"I-\")) else label\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        all_scores = []\n",
    "        # 首先添加当前 token 的分数\n",
    "        all_scores.append(probabilities[idx][pred].item())\n",
    "        _, end = offsets[idx]\n",
    "        idx += 1\n",
    "        \n",
    "        # 继续收集后续的 I- 标签\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx].item()].startswith(f\"I-{label}\")\n",
    "        ):\n",
    "            current_pred = predictions[idx].item()\n",
    "            all_scores.append(probabilities[idx][current_pred].item())\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores\n",
    "        score = np.mean(all_scores)\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": float(score),  # 确保是 Python float\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        idx += 1\n",
    "\n",
    "print(\"\\n聚合后的结果:\")\n",
    "for r in results:\n",
    "    print(f\"Entity: {r['entity_group']:10} Word: '{r['word']:15}' Score: {r['score']:.4f} [{r['start']}:{r['end']}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0ab789a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (0, 2),\n",
       " (3, 7),\n",
       " (8, 10),\n",
       " (11, 12),\n",
       " (12, 14),\n",
       " (14, 16),\n",
       " (16, 18),\n",
       " (19, 22),\n",
       " (23, 24),\n",
       " (25, 29),\n",
       " (30, 32),\n",
       " (33, 35),\n",
       " (35, 40),\n",
       " (41, 45),\n",
       " (46, 48),\n",
       " (49, 57),\n",
       " (57, 58),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "inputs_with_offsets[\"offset_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d1f1c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yl'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[12:14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c6e8e4c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "tensor([0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0])",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m idx < \u001b[38;5;28mlen\u001b[39m(predictions):\n\u001b[32m     10\u001b[39m     pred = predictions[idx]\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     label = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m label != \u001b[33m\"\u001b[39m\u001b[33mO\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     13\u001b[39m         \u001b[38;5;66;03m# Remove the B- or I-\u001b[39;00m\n\u001b[32m     14\u001b[39m         label = label[\u001b[32m2\u001b[39m:]\n",
      "\u001b[31mKeyError\u001b[39m: tensor([0, 0, 0, 0, 4, 4, 4, 4, 0, 0, 0, 0, 6, 6, 6, 0, 8, 0, 0])"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "results = []\n",
    "inputs_with_offsets = tokenizer(example, return_offsets_mapping=True)\n",
    "tokens = inputs_with_offsets.tokens()\n",
    "offsets = inputs_with_offsets[\"offset_mapping\"]\n",
    "\n",
    "idx = 0\n",
    "while idx < len(predictions):\n",
    "    pred = predictions[idx]\n",
    "    label = model.config.id2label[pred]\n",
    "    if label != \"O\":\n",
    "        # Remove the B- or I-\n",
    "        label = label[2:]\n",
    "        start, _ = offsets[idx]\n",
    "\n",
    "        # Grab all the tokens labeled with I-label\n",
    "        all_scores = []\n",
    "        while (\n",
    "            idx < len(predictions)\n",
    "            and model.config.id2label[predictions[idx]] == f\"I-{label}\"\n",
    "        ):\n",
    "            all_scores.append(probabilities[idx][pred])\n",
    "            _, end = offsets[idx]\n",
    "            idx += 1\n",
    "\n",
    "        # The score is the mean of all the scores of the tokens in that grouped entity\n",
    "        score = np.mean(all_scores).item()\n",
    "        word = example[start:end]\n",
    "        results.append(\n",
    "            {\n",
    "                \"entity_group\": label,\n",
    "                \"score\": score,\n",
    "                \"word\": word,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "            }\n",
    "        )\n",
    "    idx += 1\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de666e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde87ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
