{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092e112f",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f215da2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a6bb3ed16c4fe3b1f575c43553a2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1043917 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8567 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3dce9f802f4c1fb62a0cdb884260e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1043917 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31746e18f7640e0be30a5ce253b66f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/54948 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f80e1bdd99480bae0b413c2d032e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/54948 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Trackio project initialized: huggingface\n",
      "* Trackio metrics logged to: /root/autodl-tmp/huggingface/trackio\n",
      "* View dashboard by running in your terminal:\n",
      "\u001b[1m\u001b[93mtrackio show --project \"huggingface\"\u001b[0m\n",
      "* or by running in Python: trackio.show(project=\"huggingface\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 2:09:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.602900</td>\n",
       "      <td>1.587764</td>\n",
       "      <td>1.612800</td>\n",
       "      <td>131512.000000</td>\n",
       "      <td>0.627473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.549000</td>\n",
       "      <td>1.516506</td>\n",
       "      <td>1.529900</td>\n",
       "      <td>266413.000000</td>\n",
       "      <td>0.641325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.306900</td>\n",
       "      <td>1.492533</td>\n",
       "      <td>1.475480</td>\n",
       "      <td>383164.000000</td>\n",
       "      <td>0.645330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.535500</td>\n",
       "      <td>1.476815</td>\n",
       "      <td>1.501044</td>\n",
       "      <td>507210.000000</td>\n",
       "      <td>0.647983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.503700</td>\n",
       "      <td>1.467190</td>\n",
       "      <td>1.490720</td>\n",
       "      <td>634005.000000</td>\n",
       "      <td>0.648918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.451700</td>\n",
       "      <td>1.459018</td>\n",
       "      <td>1.479832</td>\n",
       "      <td>759575.000000</td>\n",
       "      <td>0.650634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.448100</td>\n",
       "      <td>1.453230</td>\n",
       "      <td>1.445135</td>\n",
       "      <td>884080.000000</td>\n",
       "      <td>0.651683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.423800</td>\n",
       "      <td>1.447176</td>\n",
       "      <td>1.472528</td>\n",
       "      <td>1007609.000000</td>\n",
       "      <td>0.652590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.423700</td>\n",
       "      <td>1.441910</td>\n",
       "      <td>1.460203</td>\n",
       "      <td>1138891.000000</td>\n",
       "      <td>0.653445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.471200</td>\n",
       "      <td>1.438264</td>\n",
       "      <td>1.455391</td>\n",
       "      <td>1273741.000000</td>\n",
       "      <td>0.654492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.330900</td>\n",
       "      <td>1.434624</td>\n",
       "      <td>1.438045</td>\n",
       "      <td>1407228.000000</td>\n",
       "      <td>0.654950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.308400</td>\n",
       "      <td>1.431923</td>\n",
       "      <td>1.457746</td>\n",
       "      <td>1530105.000000</td>\n",
       "      <td>0.655361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.525900</td>\n",
       "      <td>1.428527</td>\n",
       "      <td>1.435254</td>\n",
       "      <td>1658735.000000</td>\n",
       "      <td>0.656015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.263700</td>\n",
       "      <td>1.426559</td>\n",
       "      <td>1.429884</td>\n",
       "      <td>1796625.000000</td>\n",
       "      <td>0.656344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.387300</td>\n",
       "      <td>1.424239</td>\n",
       "      <td>1.434080</td>\n",
       "      <td>1922254.000000</td>\n",
       "      <td>0.656726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.417700</td>\n",
       "      <td>1.422443</td>\n",
       "      <td>1.446120</td>\n",
       "      <td>2055726.000000</td>\n",
       "      <td>0.657122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.429600</td>\n",
       "      <td>1.420959</td>\n",
       "      <td>1.432136</td>\n",
       "      <td>2188609.000000</td>\n",
       "      <td>0.657443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.286300</td>\n",
       "      <td>1.420089</td>\n",
       "      <td>1.436919</td>\n",
       "      <td>2312713.000000</td>\n",
       "      <td>0.657654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.522500</td>\n",
       "      <td>1.419176</td>\n",
       "      <td>1.441081</td>\n",
       "      <td>2444769.000000</td>\n",
       "      <td>0.657732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.486800</td>\n",
       "      <td>1.418908</td>\n",
       "      <td>1.435210</td>\n",
       "      <td>2568868.000000</td>\n",
       "      <td>0.657857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Run finished. Uploading logs to Trackio Space: http://127.0.0.1:7860/ (please wait...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.4609604654312134, metrics={'train_runtime': 7790.8407, 'train_samples_per_second': 0.513, 'train_steps_per_second': 0.128, 'total_flos': 2478923753836032.0, 'train_loss': 1.4609604654312134, 'epoch': 0.0038317112422407845})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"HuggingFaceTB/smoltalk\", \"all\")\n",
    "\n",
    "# Configure model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_name).to(\n",
    "    device\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n",
    "\n",
    "# Setup chat template\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Configure trainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eftljf2ml",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从检查点加载模型...\n",
      "✅ 已从 ./sft_output/checkpoint-1000 加载模型\n",
      "正在保存模型到 ./sft_output/final_model...\n",
      "✅ 模型已保存\n",
      "✅ README.md 已创建\n",
      "\n",
      "开始清理旧的检查点...\n",
      "  ✅ 已删除: ./sft_output/checkpoint-100\n",
      "  ✅ 已删除: ./sft_output/checkpoint-200\n",
      "  ✅ 已删除: ./sft_output/checkpoint-300\n",
      "  ✅ 已删除: ./sft_output/checkpoint-400\n",
      "  ✅ 已删除: ./sft_output/checkpoint-500\n",
      "  ✅ 已删除: ./sft_output/checkpoint-600\n",
      "  ✅ 已删除: ./sft_output/checkpoint-700\n",
      "  ✅ 已删除: ./sft_output/checkpoint-800\n",
      "  ✅ 已删除: ./sft_output/checkpoint-900\n",
      "\n",
      "✅ 清理完成！删除了 9 个检查点，只保留了 checkpoint-1000\n",
      "\n",
      "✅ 所有任务完成！模型已保存到: ./sft_output/final_model\n"
     ]
    }
   ],
   "source": [
    "# 上传模型到 Hugging Face Hub\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# 配置参数\n",
    "repo_name = \"smoltalk-sft-135M-chinese\"  # 修改为你的仓库名称\n",
    "checkpoint_path = \"./sft_output/checkpoint-1000\"  # 最新的检查点\n",
    "final_model_path = \"./sft_output/final_model\"\n",
    "\n",
    "# 从检查点加载模型和tokenizer\n",
    "print(\"正在从检查点加载模型...\")\n",
    "try:\n",
    "    # 如果之前已经训练过，从检查点加载\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        model = AutoModelForCausalLM.from_pretrained(checkpoint_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "        print(f\"✅ 已从 {checkpoint_path} 加载模型\")\n",
    "    else:\n",
    "        print(\"❌ 检查点不存在，请先运行训练代码\")\n",
    "        raise FileNotFoundError(f\"找不到检查点: {checkpoint_path}\")\n",
    "        \n",
    "except NameError:\n",
    "    # 如果 model 没有定义，尝试从检查点加载\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        model = AutoModelForCausalLM.from_pretrained(checkpoint_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "        print(f\"✅ 已从 {checkpoint_path} 加载模型\")\n",
    "    else:\n",
    "        print(\"❌ 请先运行训练代码生成模型\")\n",
    "\n",
    "# 保存最终模型和tokenizer\n",
    "print(f\"正在保存模型到 {final_model_path}...\")\n",
    "model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "print(\"✅ 模型已保存\")\n",
    "\n",
    "# 创建中文 README\n",
    "readme_content = \"\"\"---\n",
    "language: zh\n",
    "license: apache-2.0\n",
    "tags:\n",
    "- text-generation\n",
    "- causal-lm\n",
    "- sft\n",
    "- chinese\n",
    "datasets:\n",
    "- HuggingFaceTB/smoltalk\n",
    "base_model: HuggingFaceTB/SmolLM2-135M\n",
    "widget:\n",
    "- text: \"你好，请问\"\n",
    "---\n",
    "\n",
    "# SmolTalk SFT 135M 中文对话模型\n",
    "\n",
    "## 模型描述\n",
    "\n",
    "这是一个基于 SmolLM2-135M 模型使用 SFT (Supervised Fine-Tuning) 方法在 SmolTalk 数据集上微调的中文对话模型。\n",
    "\n",
    "## 训练信息\n",
    "\n",
    "- **基础模型**: HuggingFaceTB/SmolLM2-135M\n",
    "- **训练数据集**: HuggingFaceTB/smoltalk\n",
    "- **训练步数**: 1000 steps\n",
    "- **批次大小**: 4\n",
    "- **学习率**: 5e-5\n",
    "- **训练框架**: TRL (Transformer Reinforcement Learning)\n",
    "\n",
    "## 使用方法\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"your-username/smoltalk-sft-135M-chinese\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 生成对话\n",
    "inputs = tokenizer(\"你好，请问\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "## 训练配置\n",
    "\n",
    "- 最大步数: 1000\n",
    "- 每设备训练批次大小: 4\n",
    "- 学习率: 5e-5\n",
    "- 日志记录步数: 10\n",
    "- 保存步数: 100\n",
    "- 评估策略: steps\n",
    "- 评估步数: 50\n",
    "\n",
    "## 性能指标\n",
    "\n",
    "- 训练损失: ~1.46\n",
    "- 训练时长: ~2小时\n",
    "- 训练样本/秒: ~0.513\n",
    "\n",
    "## 注意事项\n",
    "\n",
    "- 该模型主要用于中文对话生成任务\n",
    "- 模型参数量较小（135M），适合资源受限的场景\n",
    "- 建议根据具体应用场景进一步微调\n",
    "\n",
    "## 许可证\n",
    "\n",
    "Apache 2.0\n",
    "\n",
    "## 引用\n",
    "\n",
    "如果你使用了这个模型，请引用原始的 SmolLM2 和 SmolTalk 项目。\n",
    "\"\"\"\n",
    "\n",
    "# 保存 README\n",
    "with open(os.path.join(final_model_path, \"README.md\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"✅ README.md 已创建\")\n",
    "\n",
    "# 删除多余的检查点（只保留最后一个）\n",
    "print(\"\\n开始清理旧的检查点...\")\n",
    "checkpoints_to_delete = [\n",
    "    \"./sft_output/checkpoint-100\",\n",
    "    \"./sft_output/checkpoint-200\",\n",
    "    \"./sft_output/checkpoint-300\",\n",
    "    \"./sft_output/checkpoint-400\",\n",
    "    \"./sft_output/checkpoint-500\",\n",
    "    \"./sft_output/checkpoint-600\",\n",
    "    \"./sft_output/checkpoint-700\",\n",
    "    \"./sft_output/checkpoint-800\",\n",
    "    \"./sft_output/checkpoint-900\",\n",
    "]\n",
    "\n",
    "deleted_count = 0\n",
    "for checkpoint in checkpoints_to_delete:\n",
    "    if os.path.exists(checkpoint):\n",
    "        shutil.rmtree(checkpoint)\n",
    "        print(f\"  ✅ 已删除: {checkpoint}\")\n",
    "        deleted_count += 1\n",
    "\n",
    "if deleted_count > 0:\n",
    "    print(f\"\\n✅ 清理完成！删除了 {deleted_count} 个检查点，只保留了 checkpoint-1000\")\n",
    "else:\n",
    "    print(\"\\n✅ 没有需要清理的检查点\")\n",
    "    \n",
    "print(f\"\\n✅ 所有任务完成！模型已保存到: {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t9ix7rlc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 仓库已创建/存在: https://huggingface.co/yiwenX/smoltalk-sft-135M-chinese\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8872d5c6d2492e8e4750875b2cd609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb4218a74644efb80751c38c7b4dfc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5aeda84465547e8b99a2e58b780c00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...utput/final_model/model.safetensors:   0%|          | 12.0kB /  538MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型已成功上传到: https://huggingface.co/yiwenX/smoltalk-sft-135M-chinese\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/yiwenX/smoltalk-sft-135M-chinese', endpoint='https://huggingface.co', repo_type='model', repo_id='yiwenX/smoltalk-sft-135M-chinese')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上传到 Hugging Face Hub\n",
    "# 注意：需要先登录或设置 token\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# 方法2：直接设置 token\n",
    "hub_token = \"xxx\"\n",
    "api = HfApi(token=hub_token)\n",
    "\n",
    "# 上传模型（需要先取消上面某个方法的注释并设置 token）\n",
    "def upload_model_to_hub(repo_name, local_path=\"./sft_output/final_model\", private=False):\n",
    "    \"\"\"\n",
    "    上传模型到 Hugging Face Hub\n",
    "    \n",
    "    Args:\n",
    "        repo_name: 仓库名称，格式为 \"username/model-name\"\n",
    "        local_path: 本地模型路径\n",
    "        private: 是否设置为私有仓库\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 创建仓库\n",
    "        api = HfApi()\n",
    "        repo_url = api.create_repo(\n",
    "            repo_id=repo_name,\n",
    "            repo_type=\"model\",\n",
    "            private=private,\n",
    "            exist_ok=True\n",
    "        )\n",
    "        print(f\"✅ 仓库已创建/存在: {repo_url}\")\n",
    "        \n",
    "        # 上传整个文件夹\n",
    "        api.upload_folder(\n",
    "            folder_path=local_path,\n",
    "            repo_id=repo_name,\n",
    "            repo_type=\"model\",\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ 模型已成功上传到: https://huggingface.co/{repo_name}\")\n",
    "        return repo_url\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 上传失败: {e}\")\n",
    "        print(\"请确保：\")\n",
    "        print(\"1. 已经登录 Hugging Face (使用 login() 或设置 token)\")\n",
    "        print(\"2. repo_name 格式正确 (username/model-name)\")\n",
    "        print(\"3. 有足够的权限创建仓库\")\n",
    "        return None\n",
    "\n",
    "# 使用示例（取消注释并修改参数后运行）\n",
    "repo_name = \"yiwenX/smoltalk-sft-135M-chinese\"  # 修改为你的用户名和模型名\n",
    "upload_model_to_hub(repo_name, private=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "w4o6cwtxjt9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "正在从 ./sft_output/final_model 加载模型...\n",
      "✅ 模型加载成功!\n",
      "\n",
      "==================================================\n",
      "开始测试模型...\n",
      "==================================================\n",
      "\n",
      "[测试 1]\n",
      "输入: What is your name?\n",
      "输出: What is your name? Please provide a simple, yet concise response.\"\n",
      "\n",
      "\"Hi Emily,\" he said, his voice firm but kind. \"I'm Alex. I'm a software engineer at a startup. I've been working on this project for a while now and I wanted to share my thoughts.\"\n",
      "\n",
      "Alex was a bit hesitant at first, but he was determined to prove that he was a valuable member of the team. He was an introvert, but he had a knack\n",
      "----------------------------------------\n",
      "\n",
      "[测试 2]\n",
      "输入: How are you today?\n",
      "输出: How are you today?\n",
      "\n",
      "What is the first thing that comes to mind when you think of today?\n",
      "\n",
      "1. What are some of the most memorable experiences you’ve had?\n",
      "\n",
      "2. What are some of the most exciting or interesting experiences you’ve had?\n",
      "\n",
      "3. What is the most memorable thing that happened to you in the last week?\n",
      "\n",
      "4. What are some of the most challenging experiences you’ve had?\n",
      "\n",
      "5. What is your best memory of this\n",
      "----------------------------------------\n",
      "\n",
      "[测试 3]\n",
      "输入: What can you do?\n",
      "输出: What can you do?\n",
      "\n",
      "I'm a student at the University of the Arts, and I'm looking for advice on how to start writing a novel. I'm thinking of starting with a story about a woman who is forced to work long hours in a factory. The story could be set in the 1930s or 1940s.\n",
      "\n",
      "How do I start?\n",
      "\n",
      "How do you start a novel?\n",
      "\n",
      "How do you write a story about\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 测试最终模型\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 设置设备\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 从 final_model 文件夹加载模型\n",
    "model_path = \"./sft_output/final_model\"\n",
    "print(f\"正在从 {model_path} 加载模型...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 将模型移到设备\n",
    "model = model.to(device)\n",
    "model.eval()  # 设置为评估模式\n",
    "print(\"✅ 模型加载成功!\")\n",
    "\n",
    "# 测试函数\n",
    "def generate_response(prompt, max_length=100, temperature=0.7, top_p=0.9, do_sample=True):\n",
    "    \"\"\"\n",
    "    生成对话回复\n",
    "    \n",
    "    Args:\n",
    "        prompt: 输入提示\n",
    "        max_length: 最大生成长度\n",
    "        temperature: 控制生成的随机性（0-1，越高越随机）\n",
    "        top_p: nucleus sampling 参数\n",
    "        do_sample: 是否采样（False则使用greedy decoding）\n",
    "    \"\"\"\n",
    "    # 准备输入\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 生成\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 解码输出\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# 测试示例\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"开始测试模型...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 测试用例列表 - 3个简单英文问题\n",
    "test_prompts = [\n",
    "    \"What is your name?\",\n",
    "    \"How are you today?\",\n",
    "    \"What can you do?\",\n",
    "]\n",
    "\n",
    "# 对每个测试用例生成回复\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n[测试 {i}]\")\n",
    "    print(f\"输入: {prompt}\")\n",
    "    response = generate_response(prompt, max_length=100, temperature=0.7)\n",
    "    print(f\"输出: {response}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "jbr6u7gpz2g",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "加载原始模型进行对比...\n",
      "==================================================\n",
      "正在加载原始模型: HuggingFaceTB/SmolLM2-135M\n",
      "✅ 原始模型加载成功!\n",
      "\n",
      "==================================================\n",
      "原始模型测试结果（微调前）\n",
      "==================================================\n",
      "\n",
      "[测试 1]\n",
      "输入: What is your name?\n",
      "原始模型输出: What is your name?\n",
      "\n",
      "\n",
      "What is your birthday?\n",
      "\n",
      "\n",
      "What is your favorite color?\n",
      "\n",
      "\n",
      "What is your favorite food?\n",
      "\n",
      "\n",
      "What is your favorite type of music?\n",
      "\n",
      "\n",
      "What is your favorite sport?\n",
      "\n",
      "\n",
      "What is your favorite food?\n",
      "\n",
      "\n",
      "What is your favorite animal?\n",
      "\n",
      "\n",
      "What is your favorite book?\n",
      "\n",
      "\n",
      "What is your favorite movie?\n",
      "\n",
      "\n",
      "What is your favorite TV show?\n",
      "\n",
      "\n",
      "What is your favorite book?\n",
      "\n",
      "\n",
      "What is your\n",
      "----------------------------------------\n",
      "\n",
      "[测试 2]\n",
      "输入: How are you today?\n",
      "原始模型输出: How are you today?\n",
      "\n",
      "It is a question that I have asked myself for months, and I have never been able to get to the answer.\n",
      "\n",
      "I am trying to answer this question in this blog post, because I am not sure how to do so.\n",
      "\n",
      "In this blog post, I will try to answer this question in a way that is easy for you to understand.\n",
      "\n",
      "It is not a question that I would answer in a simple way, because it is not\n",
      "----------------------------------------\n",
      "\n",
      "[测试 3]\n",
      "输入: What can you do?\n",
      "原始模型输出: What can you do?\n",
      "\n",
      "One of the easiest ways to prevent a cold is to wash your hands often. If you have a cold, your hands should be washed thoroughly after you touch anything that has the flu virus on it. It is also recommended to wash your hands with soap and water for at least 20 seconds.\n",
      "\n",
      "To prevent colds, you should also drink plenty of water. When you’re sick, your body loses fluids. This means you should drink more water.\n",
      "----------------------------------------\n",
      "\n",
      "==================================================\n",
      "对比总结：\n",
      "- 原始模型：基础的语言生成能力\n",
      "- 微调模型：经过 SFT 训练后的对话能力\n",
      "- 可以运行上面两个 cell 对比效果差异\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 对比测试：微调前的原始模型\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"加载原始模型进行对比...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 设置设备\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 加载原始的预训练模型（微调前）\n",
    "original_model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "print(f\"正在加载原始模型: {original_model_name}\")\n",
    "\n",
    "original_model = AutoModelForCausalLM.from_pretrained(original_model_name).to(device)\n",
    "original_tokenizer = AutoTokenizer.from_pretrained(original_model_name)\n",
    "original_model.eval()\n",
    "\n",
    "print(\"✅ 原始模型加载成功!\")\n",
    "\n",
    "# 使用相同的生成函数\n",
    "def generate_original_response(prompt, max_length=100, temperature=0.7, top_p=0.9, do_sample=True):\n",
    "    inputs = original_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = original_model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=original_tokenizer.eos_token_id if original_tokenizer.eos_token_id else original_tokenizer.pad_token_id,\n",
    "            eos_token_id=original_tokenizer.eos_token_id if original_tokenizer.eos_token_id else original_tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = original_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# 对比测试相同的问题\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"原始模型测试结果（微调前）\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "test_prompts = [\n",
    "    \"What is your name?\",\n",
    "    \"How are you today?\",\n",
    "    \"What can you do?\",\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n[测试 {i}]\")\n",
    "    print(f\"输入: {prompt}\")\n",
    "    response = generate_original_response(prompt, max_length=100, temperature=0.7)\n",
    "    print(f\"原始模型输出: {response}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"对比总结：\")\n",
    "print(\"- 原始模型：基础的语言生成能力\")\n",
    "print(\"- 微调模型：经过 SFT 训练后的对话能力\")\n",
    "print(\"- 可以运行上面两个 cell 对比效果差异\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77489366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
