{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["# 掩码语言模型(MLM)蒸馏示例"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载掩码语言模型\n",
        "teacher = AutoModelForMaskedLM.from_pretrained('bert-base-uncased').to(device)\n",
        "student = AutoModelForMaskedLM.from_pretrained('distilbert-base-uncased').to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 准备数据\n",
        "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:5000]')\n",
        "tokenized = dataset.map(lambda x: tokenizer(x['text'], truncation=True, max_length=128), batched=True)\n",
        "\n",
        "# MLM数据整理器（自动添加mask）\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=True,\n",
        "    mlm_probability=0.15\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MLM蒸馏训练器\n",
        "class MLMDistillTrainer(Trainer):\n",
        "    def __init__(self, *args, teacher=None, temp=3.0, alpha=0.5, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.teacher = teacher\n",
        "        self.temp = temp\n",
        "        self.alpha = alpha\n",
        "        \n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        outputs = model(**inputs)\n",
        "        mlm_loss = outputs.loss\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            teacher_logits = self.teacher(**inputs).logits\n",
        "        \n",
        "        # 只对被mask的位置计算蒸馏损失\n",
        "        mask_indices = inputs['labels'] != -100\n",
        "        student_logits = outputs.logits[mask_indices]\n",
        "        teacher_logits = teacher_logits[mask_indices]\n",
        "        \n",
        "        distill_loss = F.kl_div(\n",
        "            F.log_softmax(student_logits / self.temp, dim=-1),\n",
        "            F.softmax(teacher_logits / self.temp, dim=-1),\n",
        "            reduction='batchmean'\n",
        "        ) * (self.temp ** 2)\n",
        "        \n",
        "        total_loss = self.alpha * distill_loss + (1 - self.alpha) * mlm_loss\n",
        "        return (total_loss, outputs) if return_outputs else total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 训练\n",
        "trainer = MLMDistillTrainer(\n",
        "    model=student,\n",
        "    teacher=teacher,\n",
        "    args=TrainingArguments(\n",
        "        output_dir='./mlm_distilled',\n",
        "        num_train_epochs=2,\n",
        "        per_device_train_batch_size=8,\n",
        "        logging_steps=100\n",
        "    ),\n",
        "    train_dataset=tokenized,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 测试掩码预测\n",
        "text = \"The capital of France is [MASK].\"\n",
        "inputs = tokenizer(text, return_tensors='pt').to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = student(**inputs)\n",
        "    predictions = outputs.logits\n",
        "\n",
        "mask_token_index = (inputs['input_ids'] == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
        "predicted_token_id = predictions[0, mask_token_index].argmax(axis=-1)\n",
        "print(f\"预测结果: {tokenizer.decode(predicted_token_id)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}